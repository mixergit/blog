## AI大模型+全文检索+tf-idf+向量数据库+我的文章 系列之6 - 科普 : 大模型到底能干什么? 如何选型? 专业术语? 资源消耗?    
          
### 作者          
digoal          
          
### 日期          
2024-07-25         
          
### 标签          
PostgreSQL , PolarDB , DuckDB , AI , macOS , ollama , docker , 数字人 , 大模型 , 微调 , prompt tunning , fine tunning , 向量数据库 , 全文检索 , tf-idf , 权重 , RAG        
          
----          
          
## 背景          
系列文章:       
- [《AI大模型+全文检索+tf-idf+向量数据库+我的文章 系列之6 - 科普 : 大模型到底能干什么? 如何选型? 专业术语? 资源消耗?》](../202407/20240725_01.md)         
- [《AI大模型+全文检索+tf-idf+向量数据库+我的文章 系列之5 - 在 Apple Silicon Mac 上微调(fine-tuning)大型语言模型(LLM) 并发布GGUF 》](../202407/20240724_01.md)        
- [《AI大模型+全文检索+tf-idf+向量数据库+我的文章 系列之4 - RAG 自动提示微调(prompt tuning)》](../202407/20240723_01.md)           
- [《AI大模型+全文检索+tf-idf+向量数据库+我的文章 系列之3 - 微调后, 我的数字人变聪明了 》](../202407/20240722_01.md)           
- [《AI大模型+全文检索+tf-idf+向量数据库+我的文章 系列之2 - 我的数字人: 4000余篇github blog文章投喂大模型中》](../202407/20240719_01.md)          
- [《AI大模型+全文检索+tf-idf+向量数据库+我的文章 系列之1 - 低配macbook成功跑AI大模型`LLama3:8b`, 感谢ollama》](../202407/20240718_01.md)          
    
  
科普 : 大模型到底能干什么? 模型已经出现等级划分了? 专业术语?   
  
## 1 大模型到底能干什么  
参考Mistral的文档, 基本涵盖了目前大模型能完成的任务.  或者参考提示工程师手册里的例子: https://www.promptingguide.ai/zh/introduction/examples   
  
https://www.promptingguide.ai/applications  
  
https://docs.mistral.ai/getting-started/models/  
  
1、Mistral Small  
- 简单的任务：分类、客户支持、翻译或文本生成。  
  
例如  
- 判断垃圾邮件  
- 简单的智能客服系统, 例如智能电话客服、政府办事客服等  
- 简单的聊天文本生成  
  
2、Mistral 8x22B  
- 中等任务，需要中等推理能力：数据提取、文档摘要、撰写电子邮件、撰写职位描述或撰写产品描述。  
  
3、Mistral Large  
- 复杂任务，需要强大的推理能力或高度专业化：合成文本生成、代码生成(包括填空和代码补全)、检索式问答（RAG）或代理、数学推理和科学发现任务。  
  
4、其他:   
- 将文本转换成 1024 维嵌入向量。向量相似检索和检索增强生成式应用(RAG)都依赖 文本to向量的转换模型。  
- 文字、音频、图像、视频的相互转换, 信息提取等.  
  
  
### 能力对比(等级划分)  
`Mistral Large` > `Mistral 8x22B` > `Mistral Small` > `Mixtral 8x7B` > `Mistral 7B`  
  
要获得越强的能力, 就需要更大的算力和内存. ollama 上的Mistral 7B大概需要4G内存. 原来我一直在玩的是最低端的大模型, 不过已经很厉害了. 当然并不是说小模型就不能做复杂任务, 只是效果可能没那么好.    
  
```  
$ ollama list  
NAME            ID            SIZE    MODIFIED         
phi3:14b        1e67dff39209  7.9 GB  6 seconds ago     
llama3.1:8b     62757c860e01  4.7 GB  13 seconds ago    
gemma2:9b       ff02c3702f32  5.4 GB  7 hours ago       
mistral:latest  f974a74358d6  4.1 GB  7 hours ago   
```  
  
### 如何选择合适的模型?
根据你的需求, 投入的算力资源选择合适的模型. 请参考:  
- https://docs.mistral.ai/getting-started/models/
  
需求和模型选择:  
- 1、不同的模型支持的上下文大小也不一样. 例如 32k, 128k, 256k, ... 通常越大的需要更多的算力, 推理能力越强, 代码生成的能力也越强.  
- 2、简单任务, 选择Mistral Small : Classification, Customer Support, or Text Generation.
- 3、中难度任务需要推理能力, 选择MoE Mistral 8x22B : Data extraction, Summarizing a Document, Writing emails, Writing a Job Description, or Writing Product Descriptions.
- 4、复杂任务需要强推理能力和高度专业化能力, 选择Mistral Large : Synthetic Text Generation, Code Generation, RAG, or Agents.
- 5、写代码的专业模型, 选择 Codestral、Codestral-Mamba. 
- 6、向量化(embedding)专业模型, 选择 Mistral Embed. 
  
模型对应的benchmark数据:   
- Mistral 7B: outperforms Llama 2 13B on all benchmarks and Llama 1 34B on many benchmarks.
- Mixtral 8x7B: outperforms Llama 2 70B on most benchmarks with 6x faster inference and matches or outperforms GPT3.5 on most standard benchmarks. It handles English, French, Italian, German and Spanish, and shows strong performance in code generation.
- Mixtral 8x22B: our most performant open model. It handles English, French, Italian, German, Spanish and performs strongly on code-related tasks. Natively handles function calling.
- Mistral Large: a cutting-edge text generation model with top-tier reasoning capabilities. It can be used for complex multilingual reasoning tasks, including text understanding, transformation, and code generation.
- Codestral: as a 22B model, Codestral sets a new standard on the performance/latency space for code generation compared to previous models used for coding.
- Codestral-Mamba: we have trained this model with advanced code and reasoning capabilities, enabling the model to have a strong performance on par with SOTA transformer-based models.
- Mathstral: Mathstral stands on the shoulders of Mistral 7B and specialises in STEM subjects. It achieves state-of-the-art reasoning capacities in its size category across various industry-standard benchmarks.
- Mistral Nemo: Mistral Nemo's reasoning, world knowledge, and coding performance are state-of-the-art in its size category. As it relies on standard architecture, Mistral Nemo is easy to use and a drop-in replacement in any system using Mistral 7B that it supersedes.  
  
   
mistral是一个开源+商业的大模型, 手册写得还不错, 作为LLM入门学习是的不错选择.    
  
mistral 模型用法参考:  
- https://docs.mistral.ai/getting-started/stories/    
  
## 2 专业术语  
  
https://docs.mistral.ai/getting-started/glossary/  
  
1、`LLM` 大型语言模型   
  
大型语言模型 (LLM)，例如 Mistral AI 模型，是在海量文本数据上训练的 AI 模型，旨在预测句子中的下一个词。它们能够理解和生成文本的方式类似于人类沟通，可以回答问题、起草文件、总结文本、提取信息、翻译语言、编写代码等等。  
  
2、`Text generation` 文本生成  
  
大型语言模型中的文本生成是指根据给定的输入提示生成连贯且上下文相关的文本。这些模型，例如 Mistral AI，在海量文本数据上训练，通过提供前文作为上下文来预测句子中的下一个词。这种能力使它们能够生成类似人类沟通的文本，可用于各种应用，包括回答问题、起草文件、总结文本、翻译语言和编码。   
  
3、`Tokens` 文本令牌  
  
令牌(Tokens)是语言模型处理的最小单位，通常代表单词或子词等常见字符序列。为了使语言模型理解文本，必须将其转换为数值表示。 这可以通过将文本编码成一系列令牌来实现，每个令牌都被分配一个唯一的数字索引。 将文本转换为令牌的过程称为标记化或分词(tokenization)。    
  
一种广泛使用的分词算法是字节对编码（BPE），它最初将文本中的每个字节视为单独的令牌。 然后，BPE 迭代地向词汇表中添加最常见令牌对的新令牌，用新的令牌替换对的出现，直到不再进行替换为止。 这对于语言模型处理文本产生了紧凑且有效的表示。  
  
  
4、`Mixture of Experts (MoE)` 专家混合  
  
专家混合（MoE）是 `Mixtral 8x7b` 和 `Mixtral 8x22b` 的基础架构。它是一种神经网络架构，在 Transformer 块中包含专家层，允许模型以更低的计算量进行预训练，同时保持与密集模型相同的质量。  
  
通过用稀疏的 MoE 层替换密集的前馈网络 (FFN) 层来实现这一点，MoE 层包含多个“专家”（FFN）。一个门控网络或路由器决定将哪些输入tokens发送到哪个专家进行计算。  
  
MoE 提供了高效的预训练和更快的推理等优势，但也带来了挑战，例如在微调期间过拟合以及高内存需求。然而，MoE 通过动态地将输入标记分配给专门的专家进行处理，是一种实现降低计算成本的同时提高模型质量的有价值方法。  
  
MoE训练耗费更多资源但是得到的模型在同等质量下更省资源.    
  
MoE还有点像几类综合型人才的思路, 和其他模型对比的时候可以使用田忌赛马的策略, 例如搞开发里面最懂数据库的, DBA里面最懂开发的.   
  
5、`RAG` 检索增强生成  
  
检索增强生成（RAG）是一种 AI 框架，将大型语言模型 (LLM) 和信息检索系统的能力结合起来。 RAG 的主要步骤有：  
1) 检索：从向量知识库中检索与问题相关的信息； `select text as 相关信息 from t_knowledge order by text_s_vector(文本对应的向量) <=> '问题对应的向量' limit x;`  
2) 生成： 将相关信息插入到提示中，以便 LLM 结合相关信息和问题输出结果。   
  
RAG 有用于回答问题或生成利用外部知识（包括最新信息和特定领域的信息）的内容。 RAG 使模型能够访问并利用其训练数据之外的信息，从而减少幻觉并提高大模型回复的准确性。    
  
6、`Fine-tuning` 微调  
  
微调是大型语言模型中的一种过程，用于预训练模型适应特定任务或领域。它涉及在较小的、特定于任务的数据集上继续训练的过程，并调整模型的参数以优化其在新数据集上的性能/质量。 这使模型通过强化学习在特定任务场景的相关素材，提高目标任务上的性能/质量。  
  
微调与从头开始训练模型相比，微调可以在更少的数据和计算资源下实现最先进的性能/质量。  
   
即使是超大参数的模型, 如果要让其某个方向特别专业, 也可以使用微调手段来达到.   
  
7、`Function calling` 函数调用  
  
函数调用允许 Mistral 模型连接到外部工具并调用外部函数或 API，以执行超出模型自身能力的任务。 这使模型能够访问和利用外部工具和资源来提高其性能并提供更准确的响应。 函数调用可用于诸如检索实时数据、执行计算、访问数据库以及与其他系统或服务交互等任务。 它提高了模型的准确性、效率和通用性。 查看我们的 [Function Calling](https://docs.mistral.ai/capabilities/function_calling/) 指南以了解更多信息。  
  
8、`Embeddings` 嵌入/向量  
  
嵌入/向量是文本的矢量表示，通过它们在高维向量空间中的位置捕获段落的语义含义。 这些向量捕捉文本的语义含义和上下文，使模型能够更有效地理解和生成语言。 Mistral AI Embedding API 提供了用于文本的前沿、最先进的嵌入，可用于许多 NLP 任务。 查看我们的 [Embedding](https://docs.mistral.ai/capabilities/embeddings/) 指南以了解更多信息。  
  
## 3 资源消耗
模型参数大小,活跃参数,推理所需GPU内存的关系.   
  
https://docs.mistral.ai/getting-started/open_weight_models/  
  
Name  | Number of parameters  | Number of active parameters | Min. GPU RAM for inference (GB)
---|---|---|---
Mistral-7B-v0.3 |7.3B  |7.3B | 16
Mixtral-8x7B-v0.1 |46.7B |12.9B |100
Mixtral-8x22B-v0.3  |140.6B  |39.1B |300
Codestral-22B-v0.1  |22.2B |22.2B |60
Codestral-Mamba-7B-v0.1| 7.3B | 7.3B | 16
Mathstral-7B-v0.1 |7.3B  |7.3B | 16
Mistral-Nemo-Instruct-2407| 12B| 12B| 28 - bf16 </br> 16 - fp8
Mistral-Large-Instruct-2407 |123B | 123B | 228
  
## 参考  
https://www.promptingguide.ai/applications  
  
如何选型?    
- https://docs.mistral.ai/getting-started/models/  
  
https://docs.mistral.ai/getting-started/glossary/  
  
  
#### [期望 PostgreSQL|开源PolarDB 增加什么功能?](https://github.com/digoal/blog/issues/76 "269ac3d1c492e938c0191101c7238216")
  
  
#### [PolarDB 开源数据库](https://openpolardb.com/home "57258f76c37864c6e6d23383d05714ea")
  
  
#### [PolarDB 学习图谱](https://www.aliyun.com/database/openpolardb/activity "8642f60e04ed0c814bf9cb9677976bd4")
  
  
#### [购买PolarDB云服务折扣活动进行中, 55元起](https://www.aliyun.com/activity/new/polardb-yunparter?userCode=bsb3t4al "e0495c413bedacabb75ff1e880be465a")
  
  
#### [PostgreSQL 解决方案集合](../201706/20170601_02.md "40cff096e9ed7122c512b35d8561d9c8")
  
  
#### [德哥 / digoal's Github - 公益是一辈子的事.](https://github.com/digoal/blog/blob/master/README.md "22709685feb7cab07d30f30387f0a9ae")
  
  
#### [About 德哥](https://github.com/digoal/blog/blob/master/me/readme.md "a37735981e7704886ffd590565582dd0")
  
  
![digoal's wechat](../pic/digoal_weixin.jpg "f7ad92eeba24523fd47a6e1a0e691b59")
  
