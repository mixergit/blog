## AI大模型+全文检索+tf-idf+向量数据库+我的文章 系列之8 - 大模型(LLM)常见参数设置, 影响性能和可靠性  
                                                                
### 作者                                    
digoal                                    
                                           
### 日期                                         
2024-07-31                                    
                                        
### 标签                                      
PostgreSQL , PolarDB , DuckDB , 大模型 , 参数设置 , temperature , top_p , num_ctx             
                                                               
----                                        
                                                      
## 背景       
系列文章:         
- [《AI大模型+全文检索+tf-idf+向量数据库+我的文章 系列之8 - 大模型(LLM)常见参数设置, 影响性能和可靠性》](../202407/20240731_02.md)        
- [《AI大模型+全文检索+tf-idf+向量数据库+我的文章 系列之7 - 如何生成 “微调(fine-tuning)大语言模型(LLM)” 的问题素材?》](../202407/20240730_01.md)        
- [《AI大模型+全文检索+tf-idf+向量数据库+我的文章 系列之6 - 科普 : 大模型到底能干什么? 如何选型? 专业术语? 资源消耗?》](../202407/20240725_01.md)             
- [《AI大模型+全文检索+tf-idf+向量数据库+我的文章 系列之5 - 在 Apple Silicon Mac 上微调(fine-tuning)大型语言模型(LLM) 并发布GGUF 》](../202407/20240724_01.md)            
- [《AI大模型+全文检索+tf-idf+向量数据库+我的文章 系列之4 - RAG 自动提示微调(prompt tuning)》](../202407/20240723_01.md)               
- [《AI大模型+全文检索+tf-idf+向量数据库+我的文章 系列之3 - 微调后, 我的数字人变聪明了 》](../202407/20240722_01.md)               
- [《AI大模型+全文检索+tf-idf+向量数据库+我的文章 系列之2 - 我的数字人: 4000余篇github blog文章投喂大模型中》](../202407/20240719_01.md)              
- [《AI大模型+全文检索+tf-idf+向量数据库+我的文章 系列之1 - 低配macbook成功跑AI大模型`LLama3:8b`, 感谢ollama》](../202407/20240718_01.md)              
    
你使用大模型有一些时间后, 可能会发现这些问题  
- 为什么同一个问题问大模型多次, 得到的结果不一样?  
- 为什么有时大模型的回答感觉很啰嗦, 不够聚焦和干练?  
- 为什么有时一个问题没有完全回复完, 大模型就停止返回了?  
- 为什么有时大模型感觉很有想象力, 天马行空, 有时又非常拘谨, 一点创造力都没有?  
  
这些其实和大模型的参数有关, 可以参考如下文档的介绍  
- https://docs.mistral.ai/api  
- https://www.promptingguide.ai/zh/introduction/settings  
- https://github.com/ollama/ollama/blob/main/docs/modelfile.md#parameter  
- https://github.com/ollama/ollama/tree/main/docs  
  
ollama文档中关于llm参数的介绍非常的详细  
  
| Parameter      | Description                                                                                                                                                                                                                                             | Value Type | Example Usage        |  
| -------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------- | -------------------- |  
| mirostat       | 启用 Mirostat 采样来控制困惑度。 (default: 0, 0 = disabled, 1 = Mirostat, 2 = Mirostat 2.0)                                                                                                                                         | int        | mirostat 0           |  
| mirostat_eta   | 生成文本反馈的响应速度, 较低的值导致较慢的生成响应速度，较高的值导致较快的生成响应速度。 (Default: 0.1)                        | float      | mirostat_eta 0.1     |  
| mirostat_tau   | 控制输出的连贯性和多样性之间的平衡。值越低，文本内容越聚焦、越连贯。(Default: 5.0)                                                                                                         | float      | mirostat_tau 5.0     |  
| num_ctx        | 生成下一个token时, 在已生成内容中, 从当前位置回顾上下文窗口的token数。 (Default: 2048)                                                                                                                                                                    | int        | num_ctx 4096         |  
| repeat_last_n  | 模型回溯多远(上下文窗口)以防止生成重复token。 (Default: 64, 0 = disabled, -1 = num_ctx)                                                                                                                                           | int        | repeat_last_n 64     |  
| repeat_penalty | 对生成重复token的惩罚力度。较高的值（例如 1.5）将对重复的惩罚力度更大，而较低的值（例如 0.9）将更宽松。（默认值：1.1）                                                                     | float      | repeat_penalty 1.1   |  
| temperature    | 模型的温度。头脑发热原来是这么解释的. 增加温度将使模型的回答更具创意(开放), 适合需要创意的工作例如写作. 更低的温度使得回答更加精准, 适合有标准答案的场景(例如产品手册助手)。 (Default: 0.8)                                                                                                                                     | float      | temperature 0.7      |  
| seed           | 设置用于生成的随机数种子。将其设置为特定数字将使模型针对同一提示生成相同的文本。（默认值：0）                                                                                      | int        | seed 42              |  
| stop           | 设置停止词。遇到此停止词时，LLM 将停止生成文本并返回。可以通过`stop`在`modelfile`中定义多个单独的停止词。 (生成文本的过程中,遇到stop设定的词则停止. 例如你把stop设置为"的"字, 可能很容易就停止输出, 因为“的”字使用频率非常高. )                                    | string     | stop "AI assistant:" |   
| tfs_z          | 减少输出中 “可能性较小的token” 的影响 (对不关键信息的减权, 例如生成token参考的上下文中的不关键信息)。较高的值（例如 2.0）将进一步减少影响，而值 1.0 则禁用此设置。                                              | float      | tfs_z 1              |  
| num_predict    | 生成文本时要返回的最大token数。 (Default: 128, -1 = infinite generation, -2 = fill context)                                                                                                                                   | int        | num_predict 42       |  
| top_k          | 降低产生无意义答案的概率。值越高（例如 100）答案就越多样化，值越低（例如 10）答案就越保守。（默认值：40）                                                                       | int        | top_k 40             |  
| top_p          | 与 top-k 配合使用。较高的值（例如 0.95）将产生更加多样化的文本，而较低的值（例如 0.5）将产生更加集中和保守的文本。（默认值：0.9）                                                                  | float      | top_p 0.9            |  
| min_p          | top_p 的替代方案，旨在确保质量和多样性的平衡。参数p表示相对于最可能标记的概率，考虑标记的最小概率。例如，当p =0.05 且最可能标记的概率为 0.9 时，值小于 0.045 的 logit 将被过滤掉。（默认值：0.0） | float      | min_p 0.05            |  
  
  
  
## 使用ollama体验一下  
ollama 命令行可以通过command来设置参数.   
  
### 先来熟悉一下命令行的用法.   
  
获得command帮助  
```  
>>> /?  
Available Commands:  
  /set            Set session variables  
  /show           Show model information  
  /load <model>   Load a session or model  
  /save <model>   Save your current session  
  /clear          Clear session context  
  /bye            Exit  
  /?, /help       Help for a command  
  /? shortcuts    Help for keyboard shortcuts  
  
Use """ to begin a multi-line message.  
```  
  
设置用法  
```  
>>> /set  
Available Commands:  
  /set parameter ...     Set a parameter  
  /set system <string>   Set system message  
  /set history           Enable history  
  /set nohistory         Disable history  
  /set wordwrap          Enable wordwrap  
  /set nowordwrap        Disable wordwrap  
  /set format json       Enable JSON mode  
  /set noformat          Disable formatting  
  /set verbose           Show LLM stats  
  /set quiet             Disable LLM stats  
```  
  
设置参数用法  
```  
>>> /set parameter  
Available Parameters:  
  /set parameter seed <int>             Random number seed  
  /set parameter num_predict <int>      Max number of tokens to predict  
  /set parameter top_k <int>            Pick from top k num of tokens  
  /set parameter top_p <float>          Pick token based on sum of probabilities  
  /set parameter num_ctx <int>          Set the context size  
  /set parameter temperature <float>    Set creativity level  
  /set parameter repeat_penalty <float> How strongly to penalize repetitions  
  /set parameter repeat_last_n <int>    Set how far back to look for repetitions  
  /set parameter num_gpu <int>          The number of layers to send to the GPU  
  /set parameter stop <string> <string> ...   Set the stop parameters  
```  
  
列出配置  
```  
>>> /show  
Available Commands:  
  /show info         Show details for this model  
  /show license      Show model license  
  /show modelfile    Show Modelfile for this model  
  /show parameters   Show parameters for this model  
  /show system       Show system message  
  /show template     Show prompt template  
```  
  
列出已定义的参数  
```  
>>> /show parameters  
Model defined parameters:  
stop                           "<|start_header_id|>"  
stop                           "<|end_header_id|>"  
stop                           "<|eot_id|>"  
```  
  
### 例子  
1、停止词的使用  
```  
>>> /set parameter stop 太阳  
Set parameter 'stop' to '太阳'  
>>> 请使用简体中文回答天上有几个太阳  
有很多个这样的  
```  
  
2、为什么有时一个问题没有完全回复完, 大模型就停止返回了?  
```  
>>> /set parameter num_predict 10    
Set parameter 'num_predict' to '10'  
>>> 请使用简体中文回答天上有几个太阳  
看来你也很感兴趣!  
  
说  
```  
  
3、为什么有时大模型感觉很有想象力, 天马行空, 有时又非常拘谨, 一点创造力都没有?  
  
温度低, 偏拘谨  
```  
>>> /set parameter temperature 0.1  
Set parameter 'temperature' to '0.1'  
>>> 写一首赞美妇女的诗    
美丽的女性，  
  
你是天空中的星星，  
你是阳光下的花朵，  
你是温暖的春风。  
  
你的笑容可以照亮整个世界，  
你的眼睛像清澈的泉水，  
你的声音像婉转的曲调。  
  
你是母亲，带来生命和希望，  
你是妻子，带来爱情和温暖，  
你是朋友，带来支持和理解。  
  
你是天空中的云朵，  
你是阳光下的雨滴，  
你是温暖的夏日。  
  
美丽的女性，  
  
你是世界上最美丽的宝贝。  
```  
  
温度高, 更具有创造力  
```  
>>> /set parameter temperature 1000  
Set parameter 'temperature' to '1000'  
>>> 写一首赞美妇女的诗  
女性啊，世界上的美好，  
  
像绚丽的大天空般，她闪光奪目的，  
  
她能为生活带来温馨与幸福，  
  
在人间，也是春日花儿开放的地方。  
  
在阳光下，面床的妩媚，  
散发出清凉的风；  
  
那倩倩柔若无骨的身材，  
  
透射出女性的一切特点；  
  
你比金子的宝贝，  
  
更加精致，也更加耀眼。  
```  
  
4、为什么有时大模型的回答感觉很啰嗦, 不够聚焦和干练?  
  
mirostat_tau值低, 聚焦、干练  
```  
>>> /set parameter mirostat_tau 0.1  
Set parameter 'mirostat_tau' to '0.1'  
>>> 请使用简体中文回答天上有几个太阳  
实际上，每一颗恒心在自己体系里都有一颗“太阳”。  
  
那么问题来了：这是在我们的整个恒星体系中每个恒星都是独一无二的。  
```  
  
mirostat_tau值高, 啰嗦  
```  
>>> /set parameter mirostat_tau 1000  
Set parameter 'mirostat_tau' to '1000'  
>>> 请使用简体中文回答天上有几个太阳  
那么你问得好啊！  
  
事实上，我们能看到每一个恒心体系中的只有1颗。  
  
不过，它们也许有一颗比较大且亮光，而我们自己却只有一颗日子！  
  
这可能是很常见的一个特点，也有许多这样不明显的小变化存在，但它们就是非常特殊和重要。  
```  
     
## 参考  
- https://www.promptingguide.ai/zh/introduction/settings  
- https://github.com/ollama/ollama/blob/main/docs/modelfile.md#parameter  
- https://github.com/ollama/ollama/tree/main/docs  
  
  
  
#### [期望 PostgreSQL|开源PolarDB 增加什么功能?](https://github.com/digoal/blog/issues/76 "269ac3d1c492e938c0191101c7238216")
  
  
#### [PolarDB 开源数据库](https://openpolardb.com/home "57258f76c37864c6e6d23383d05714ea")
  
  
#### [PolarDB 学习图谱](https://www.aliyun.com/database/openpolardb/activity "8642f60e04ed0c814bf9cb9677976bd4")
  
  
#### [购买PolarDB云服务折扣活动进行中, 55元起](https://www.aliyun.com/activity/new/polardb-yunparter?userCode=bsb3t4al "e0495c413bedacabb75ff1e880be465a")
  
  
#### [PostgreSQL 解决方案集合](../201706/20170601_02.md "40cff096e9ed7122c512b35d8561d9c8")
  
  
#### [德哥 / digoal's Github - 公益是一辈子的事.](https://github.com/digoal/blog/blob/master/README.md "22709685feb7cab07d30f30387f0a9ae")
  
  
#### [About 德哥](https://github.com/digoal/blog/blob/master/me/readme.md "a37735981e7704886ffd590565582dd0")
  
  
![digoal's wechat](../pic/digoal_weixin.jpg "f7ad92eeba24523fd47a6e1a0e691b59")
  
