## AI大模型+全文检索+tf-idf+向量数据库+我的文章 系列之5 - 在 Apple Silicon Mac 上微调(fine-tuning)大型语言模型(LLM) 并发布GGUF   
        
### 作者        
digoal        
        
### 日期        
2024-07-24        
        
### 标签        
PostgreSQL , PolarDB , DuckDB , AI , macOS , ollama , docker , 数字人 , 大模型 , 微调 , prompt tunning , fine tunning , 向量数据库 , 全文检索 , tf-idf , 权重 , RAG      
        
----        
        
## 背景        
系列文章:       
- [《AI大模型+全文检索+tf-idf+向量数据库+我的文章 系列之5 - 在 Apple Silicon Mac 上微调(fine-tuning)大型语言模型(LLM) 并发布GGUF 》](../202407/20240724_01.md)      
- [《AI大模型+全文检索+tf-idf+向量数据库+我的文章 系列之4 - RAG 自动提示微调(prompt tuning)》](../202407/20240723_01.md)         
- [《AI大模型+全文检索+tf-idf+向量数据库+我的文章 系列之3 - 微调后, 我的数字人变聪明了 》](../202407/20240722_01.md)         
- [《AI大模型+全文检索+tf-idf+向量数据库+我的文章 系列之2 - 我的数字人: 4000余篇github blog文章投喂大模型中》](../202407/20240719_01.md)        
- [《AI大模型+全文检索+tf-idf+向量数据库+我的文章 系列之1 - 低配macbook成功跑AI大模型`LLama3:8b`, 感谢ollama》](../202407/20240718_01.md)        
  
  
原文 (A simple guide to local LLM fine-tuning on a Mac with MLX) :   
- https://apeatling.com/articles/simple-guide-to-local-llm-fine-tuning-on-a-mac-with-mlx/  
- https://apeatling.com/articles/part-1-setting-up-your-environment/  
- https://apeatling.com/articles/part-2-building-your-training-data-for-fine-tuning/  
- https://apeatling.com/articles/part-3-fine-tuning-your-llm-using-the-mlx-framework/  
- https://apeatling.com/articles/part-4-testing-and-interacting-with-your-fine-tuned-llm/
  
微调云端的大模型saas服务:   
- https://docs.mistral.ai/capabilities/finetuning/  
- https://docs.mistral.ai/guides/finetuning/  
    
你好！您是否想在 Apple Silicon Mac 上微调大型语言模型 (LLM)？你可能想要一个某方面拔尖的LLM而不是一款中庸的LLM？如果是这样，那么您来对地方了。  
  
让我们一步一步地介绍微调过程。这不会花你一分钱，因为我们将使用[Apple 的 MLX 框架](https://github.com/ml-explore/mlx)在您自己的硬件上完成所有操作。  
  
一旦我们完成，您将拥有一个完全优化的 LLM(例如 根据常见的PostgreSQL问题和高质量答案进行微调后 得到一个非常适合解决pg问题的大模型)，您可以在自己的设备上轻松完成所有操作。  
  
我把本指南分成了多个部分。每个部分都是独立的，所以你可以直接跳到与你最相关的部分：  
- 1 设置您的环境。  
- 2 构建训练数据以进行微调。  
- 3 使用 MLX 框架微调您的 LLM。  
- 4 测试并与您微调的 LLM 进行交互。  
  
## 第 1 部分：设置你的环境  
  
如果您是从零开始，则需要在 Mac 上设置一些东西才能开始。如果您已准备好 Python 和 Pip，则可以跳至第二步。  
  
### 步骤 1：安装 Homebrew  
PS: 如果要使用国内源加速brew和pip, 可以参考: [《AI大模型+全文检索+tf-idf+向量数据库+我的文章 系列之2 - 我的数字人: 4000余篇github blog文章投喂大模型中》](../202407/20240719_01.md)      
  
[Homebrew](https://brew.sh/)是一个包管理器，可以帮助你管理和安装所需的软件（例如 Python）。我喜欢它，因为它提供了一种切换版本和卸载包的简单方法。  
  
要安装 Homebrew，请打开终端应用程序（或安装 iTerm）并粘贴以下内容：  
```  
/bin/bash -c "$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)"  
```  
  
如果安装过程中提示您，请继续安装 Xcode 命令行工具。  
  
### 步骤 2：安装升级版本的 Python 和 Pip  
  
从 Mac OS 12 Sonoma 开始，Mac OS 已预装 Python 3。我已升级到 3.11.x 以获取最新的修复和安全更新。  
  
我会坚持使用最新版本减去一个主要版本，否则您可能会遇到不兼容的库问题。因此，如果 3.12 是最新版本，请从 3.11 版本中选择最新版本。  
  
要安装 Python 3.11 的最新次要版本，请使用以下命令：  
```  
brew install python@3.11  
```  
  
完成后，您将安装 python 3.11.x 及其包管理器 pip。  
  
一个可选步骤是将python3.11和pip3.11命令别名(alias)为python和pip，这样在遵循示例时会更简单一些。版本更改时，您需要更改这些别名。  
  
您可以使用以下命令为它们添加别名（如果您安装了较新的版本，请使用较新的版本号）：  
  
将 python 和 pip 别名添加到 zsh 配置的命令  
```  
echo '  
alias python=python3.11  
alias pip=pip3.11  
' >> ~/.zshrc  
```  
  
将 python 和 pip 别名添加到 bash 配置的命令  
```  
echo '  
alias python=python3.11  
alias pip=pip3.11  
' >> ~/.bash_profile  
```  
  
这就是环境，现在您可以进入 第 2 部分：构建用于微调的训练数据。  
  
## 第 2 部分：构建用于微调的训练数据  
  
有许多不同的工具可以在 Mac 上本地运行 LLM。[LM Studio](https://lmstudio.ai/)是一个很好的工具，它提供了一个不错的 UI 来运行和与离线 LLM 聊天。在本指南中，我将使用[Ollama](https://ollama.ai/)，因为它提供了一个本地 API，我们将使用它来构建微调训练数据。  
  
### 步骤 1：下载 Ollama 并提取模型  
下载并安装 [Ollama](https://ollama.ai/download)。在本指南中，我将使用[Mistral.ai](https://mistral.ai/)的[Mistral 7B Instruct v0.2](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2) 模型。如果您愿意，可以拉取不同的模型，只需从现在开始将所有内容切换为您自己的模型即可。  
  
要拉取模型，请使用以下命令：拉下 Mistral 7B 并使用默认的指导模型。  
```  
ollama pull mistral:7b  
```  
  
一旦有了模型，您可以选择使用命令run来pull尝试从命令行与其进行交互。  
  
### 步骤 2：确定正确的训练数据格式  
为了微调 Mistral 7B，我们需要训练数据。训练数据将使微调后的模型能够产生比单靠提示更高质量的结果。您的训练数据应该充满了您希望在微调后看到的结果类型的示例。  
  
Mistral 7B 的训练数据应采用[JSONL 格式](https://jsonlines.org/)，每行格式如下：JSONL 格式的单行训练数据。  
```  
{"text":"<s>[INST] Instruction[/INST] Model answer</s>[INST] Follow-up instruction[/INST]"}  
```  
  
如果您正在训练不同的模型(例如 llama3, gemma2, ...)，格式可能会有所不同 - 因此请务必检查。  
  
如果您在电子表格、数据库或其他存储中已有训练数据，那么您的目标是将其转换为上述 JSONL 格式。如果您已完成此操作，则可以继续进行微调步骤。  
  
### 步骤 3：创建生成说明的提示(prompt)  
我手头没有任何好的训练数据，所以我决定生成它。我想微调模型，以便对有关使用 WordPress 网站的问题提供更广泛的答案。  
  
我使用了 Ollama，运行了一系列不同的模型，并使用了以下提示：  
```  
Please list in JSON format 50 frequently asked questions about WordPress from all levels of users working on WordPress websites.    
  
The questions should start with any of the following: “Where do I", "Is it okay to", "Can you help me", "I need to", "Is there a", "Do you know", "Where is the", "Can you tell me", "Can I change", "What are the", "How do I", "When is it", "Does WordPress have", "How to", "What is the difference", "Can users", "Can I", "What is”.    
  
You do not need to provide an answer or category to each question. The list should be a single dimension array of only questions.  
```  
  
经过反复尝试才得到以上提示。要求以特定短语开头的说明有助于改善生成问题的范围。  
  
最好花一些时间在这里找出最佳提示，以便生成最适合您需求的指令列表。  
  
如果你想训练一个PG专用大模型, 可以这样问大模型  
```  
你是PostgreSQL专家, 熟悉数据库原理, 了解业务如何使用数据库并发挥数据库最佳能力, 有丰富的数据库管理、实战、架构经验. 并对数据库周边生态非常熟悉.  
请问如果要让学生、数据库管理员、应用开发者、数据库内核开发者、数据库周边生态工具开发者了解PostgreSQL, 应该问一些什么问题, 请给出问题即可, 不需要答案.  
  
请给我一维数组的结果, 10个大类, 每类10个最重要的问题, 每一行1个问题.  
```  
  
### 步骤 4：使用 Ollama 生成指令(问题)  
一旦你有了良好的生成提示，你就可以通过 Ollama 运行它并切换你使用的[模型](https://ollama.ai/library)。对于每组指令，我将它们添加到单个 JSON 文件中。  
  
以下是 Mistral 7B 的截断结果：  
```  
❯ ollama run mistral:7b  
>>> Please list in JSON format 50 frequently asked questions about WordPress from all levels of users working on WordPress websites.  
...  
... The questions should start with any of the following: “Where do I", "Is it okay to", "Can you help me", "I need to", "Is there a",  
... "Do you know", "Where is the", "Can you tell me", "Can I change", "What are the", "How do I", "When is it", "Does WordPress have",  
... "How to", "What is the difference", "Can users", "Can I", "What is”.  
...  
... You do not need to provide an answer or category to each question. The list should be a single dimension array of only questions.  
 [  
"Where do I find the WordPress login url?",  
"Is it okay to install WordPress on a local server?",  
"Can you help me recover a lost WordPress password?",  
"I need to update my WordPress version, how do I do that safely?",  
"Is there a limit to the number of plugins I can install?",  
"Do you know how to create custom post types in WordPress?",  
"Where is the database location for a WordPress site?",  
"Can you tell me how to add custom CSS to my WordPress site?",  
"Can I change the permalink structure in WordPress?"  
  
[...]  
```  
  
PG问题的结果:  
```  
1. "PostgreSQL基础知识"  
   - 什么是PostgreSQL？它与其他数据库有什么区别？  
   - PostgreSQL中如何创建和使用数据库、表、索引等对象？  
   - PostgreSQL中的数据类型有哪些，以及它们之间的区别？  
   - 在PostgreSQL中如何进行基本操作，例如插入、查询、更新和删除数据？  
   - 什么是事务，并说明在PostgreSQL中如何使用事务？  
  
2. "PostgreSQL高级知识"  
   - 在PostgreSQL中如何实现存储过程和函数？它们之间的区别是怎样的？  
   - PostgreSQL中如何实现触发器，以及它们与函数、过程有什么不同？  
   - 如何在PostgreSQL中实现数据驱动式的Web应用？可以提供一些常见的技术或框架吗？  
   - PostgreSQL中如何实现对数据的加密和解密，以及它们之间的区别是怎样的？  
   - 在PostgreSQL中如何实现数据分析和报表生成？可以提供一些常见的工具或方法吗？  
  
3. "PostgreSQL性能优化"  
   - 在使用PostgreSQL时，应该如何设计索引来最大化查询性能？有哪些常见的错误和优化技巧？  
   - PostgreSQL中如何配置参数以提高性能？有哪些重要的参数需要注意？  
   - 如何在PostgreSQL中使用视图来提高数据访问效率？  
   - 如何在PostgreSQL中实现数据复制和镜像，以及它们之间的区别是怎样的？  
   - PostgreSQL中如何监控和分析性能，以及它们之间的区别是怎样的？  
  
4. "PostgreSQL安全"  
   - 在PostgreSQL中如何实现对数据的加密和解密，以及它们之间的区别是怎样的？  
   - PostgreSQL中如何设置用户和权限来保护数据？有哪些重要的安全原则需要注意？  
   - 在PostgreSQL中如何实现对数据库和表的备份和还原，以及它们之间的区别是怎样的？  
   - PostgreSQL中如何实现对数据库的监控和日志记录，以及它们之间的区别是怎样的？  
   - 在PostgreSQL中如何配置防火墙和网络安全策略，以保护数据库安全？  
  
5. "PostgreSQL实战"  
   - PostgreSQL中如何处理大量数据，例如批量插入、更新和删除？有哪些技巧可以提高性能？  
   - 在PostgreSQL中如何处理空间类型数据，例如地图、图像和视频等？有哪些常见的技术或工具需要注意？  
   - PostgreSQL中如何处理时序数据，例如日志记录和性能监控？有哪些特定的功能需要注意？  
   - 在PostgreSQL中如何处理事务多方面问题，例如两阶段提交、可靠消息等？有哪些技术或工具需要注意？  
   - PostgreSQL中如何处理实时数据流和消息队列？有哪些常见的技术或工具需要注意？  
  
6. "PostgreSQL架构设计"  
   - 在PostgreSQL中如何设计数据库架构，以满足不同业务场景的需求？  
   - PostgreSQL中如何实现多库和库之间的通信，以及它们之间的区别是怎样的？  
   - PostgreSQL中如何实现分片和分布式数据存储，以及它们之间的区别是怎样的？  
   - 在PostgreSQL中如何实现数据库高可用性和灾难恢复，以及它们之间的区别是怎样的？  
   - PostgreSQL中如何实现数据库扩展性和可扩展性，以及它们之间的区别是怎样的？  
  
7. "PostgreSQL工具使用"  
   - 什么是pgAdmin，并说明它在PostgreSQL开发过程中的重要性？  
   - 什么是psql，并说明它在PostgreSQL开发过程中的重要性？  
   - 什么是pg\_dump和pg\_restore，以及它们之间的区别是怎样的？  
   - 什么是PostGIS和PL/Python，以及它们在PostgreSQL开发过程中的重要性？  
   - 什么是PostgreSQL插件，以及它们在PostgreSQL开发过程中的重要性？  
  
8. "PostgreSQL应用场景"  
   - PostgreSQL在Web开发中的使用场景是怎样的？有哪些常见的技术或框架需要注意？  
   - PostgreSQL在移动开发中的使用场景是怎样的？有哪些常见的技术或工具需要注意？  
   - PostgreSQL在数据挖掘和机器学习中的使用场景是怎样的？有哪些特定的功能需要注意？  
   - PostgreSQL在 IoT 开发中的使用场景是怎样的？有哪些常见的技术或工具需要注意？  
   - PostgreSQL在大数据处理中的使用场景是怎样的？有哪些特定的功能需要注意？  
  
9. "PostgreSQL社区"  
   - PostgreSQL社区有哪些重要的资源，例如文档、视频和论坛等？  
   - PostgreSQL社区有哪些常见的问题和解决方案，以及它们之间的区别是怎样的？  
   - 在PostgreSQL社区中发现新鲜事物的途径有哪些？有哪些经验值得学习？  
   - PostgreSQL社区中的贡献者有哪些值得注意的人物，以及他们在PostgreSQL开发中的重要性？  
   - PostgreSQL社区中的活动和会议有哪些需要关注，以及它们之间的区别是怎样的？  
  
10. "PostgreSQL未来"  
    - PostgreSQL未来的发展趋势是什么？有哪些重要的项目或技术需要注意？  
    - PostgreSQL未来在云计算和容器化环境中的应用场景是怎样的？有哪些常见的技术或工具需要注意？  
    - PostgreSQL未来对数据库市场和行业的影响是什么？有哪些特定的问题需要注意？  
    - 在PostgreSQL未来中，如何发展和增长PostgreSQL社区？有哪些经验值得学习？  
    - PostgreSQL未来对开发者和用户的挑战是什么？有哪些技巧可以克服这些挑战？  
```  
  
人们普遍认为，至少需要 50-100 个示例才能通过微调获得有利结果。我无法说出理想的数量是多少，但我想你会想要更多。  
  
我继续生成了 1,200 条指令(instructions)。下次我将编写一个脚本来执行此操作，而不是手动将它们添加到 JSON 文件中。  
  
将所有问题都放在一个 JSON 文件中。保存此文件。我将其命名为`instructions.json`。  
  
`instructions.json` 里面包含了以上所有的问题, 格式很关键`[ ... 每行一个问题 ... ]`, 因为后面的代码将提取这个json里的每一行, 丢给大模型进行回复.    
  
注意了, `instructions.json` 最好请人工专家进行调整(增删改), 确保全面且有代表性, 微调出来的模型才更厉害.     
  
### 步骤 5：根据指令(问题)生成答案  
现在您拥有所有指令的 JSON 文件(`instructions.json`)，您可以使用 [Ollama API](https://github.com/jmorganca/ollama/blob/main/docs/api.md)为每个指令生成模型答案。  
  
为此，我编写了一个非常简单的 Python 脚本，可以在命令行上运行该脚本来查询 Ollama API 并生成 JSONL 训练文件。  
  
将其保存为`generate.py`文件。我在 [Github](https://github.com/apeatling/beginners-guide-to-mlx-finetuning/tree/trunk) 上有一个完整的示例副本，您还可以在其中找到该脚本的 其他 版本。  
  
这个python脚本将询问mistral来获得以上问题的答案, 并产生微调文件`<train.jsonl> , <valid.jsonl>`    
```python  
import json  
import requests  
import sys  
from pathlib import Path  
  
def query_ollama(prompt, model='mistral', context=''):  
    url = 'http://localhost:11434/api/generate'  
    data = {"model": model, "stream": False, "prompt": context+prompt}  
    response = requests.post(url, json=data)  
    response.raise_for_status()  
    followup_data = {"model": model, "stream": False, "prompt": response.json()['response'].strip() + "What is a likely follow-up question or request? Return just the text of one question or request."}  
    followup_response = requests.post(url, json=followup_data)  
    followup_response.raise_for_status()  
    return response.json()['response'].strip(), followup_response.json()['response'].replace("\"", "").strip()  
  
def create_validation_file(train_file, valid_file, split_ratio):  
    with open(train_file, 'r') as file:  
        lines = file.readlines()  
    valid_lines = lines[:int(len(lines) * split_ratio)]  
    train_lines = lines[int(len(lines) * split_ratio):]  
    with open(train_file, 'w') as file:  
        file.writelines(train_lines)  
    with open(valid_file, 'w') as file:  
        file.writelines(valid_lines)  
  
def main(instructions_file, train_file, valid_file, split_ratio):  
    if not Path(instructions_file).is_file():  
        sys.exit(f'{instructions_file} not found.')  
  
    with open(instructions_file, 'r') as file:  
        instructions = json.load(file)  
  
    for i, instruction in enumerate(instructions, start=1):  
        print(f"Processing ({i}/{len(instructions)}): {instruction}")  
        answer, followup_question = query_ollama(instruction)  
        result = json.dumps({  
            'text': f'<s>[INST] {instruction}[/INST] {answer}</s>[INST]{followup_question}[/INST]'  
        }) + "\n"  
        with open(train_file, 'a') as file:  
            file.write(result)  
      
    create_validation_file(train_file, valid_file, split_ratio)  
    print("Done! Training and validation JSONL files created.")  
  
if __name__ == "__main__":  
    if len(sys.argv) != 5:  
        sys.exit("Usage: python script.py <instructions.json> <train.jsonl> <valid.jsonl> <split_ratio>")  
    main(sys.argv[1], sys.argv[2], sys.argv[3], float(sys.argv[4]))  
```  
  
该脚本将允许您根据要用于获取答案的模型来更改模型。您甚至可以切换模型并从不同的模型生成答案。  
  
要运行脚本，您可以调用它 `python3 generate.py instructions.json train.jsonl valid.jsonl 0.6` , 相当于通过instructions汇总的问题生成有答案的结果集, 并把结果集拆成2个部分, `train.jsonl`用于训练, `valid.jsonl`用于调教/拟合训练结果, 类似监督学习的过程.   
  
生成完成后，您将获得两个新文件：`train.jsonl`和`valid.jsonl`。我们将在第三部分中使用这些文件和Apple 的 MLX 框架在您的 Mac 上启动本地模型训练。  
  
注意了, `train.jsonl`和`valid.jsonl` 最好请人工专家纠正或者加强一下. 否则就是用一个模型的结果来训练另一个模型, 没有多大意义.    
  
## 第 3 部分：使用 MLX 框架微调你的 LLM  
  
### 步骤 1：克隆 mlx-examples Github 仓库 
苹果公司的机器学习研究人员创建了一个名为 [MLX](https://github.com/ml-explore/mlx) 的出色新框架。MLX 是一个专为 Apple 芯片构建的阵列框架，允许其利用 Mac 中的统一内存，从而带来显着的性能改进。  
  
[mlx-examples](https://github.com/ml-explore/mlx-examples) 包含一系列使用 MLX 的不同示例。这是学习如何使用它的好方法。  
  
我假设您了解 Git 和 Github。您可以使用以下方法克隆 MLX 示例仓库：  
```  
git clone https://github.com/ml-explore/mlx-examples.git  
```  
  
在本指南中，我们将重点关注 [LoRA（低秩自适应 Low-Rank Adaptation）](https://huggingface.co/papers/2106.09685) [示例](https://github.com/ml-explore/mlx-examples/tree/main/lora)，我们将使用这些示例来微调 Mistral 7B Instruct 并创建我们的微调适配器。  
  
### 步骤 2：使用训练数据开始微调过程  
在 MLX 的早期版本中，您需要将模型转换为与 MLX 兼容。[现在不再需要这样做了](https://twitter.com/awnihannun/status/1744801528253382721)，您可以从 Hugging Face 中指定一个模型并立即开始使用它进行微调。  
  
我们可以使用上一步的`train.jsonl`和`valid.jsonl`文件来启动训练程序。  
  
第一步是前往`mlx-examples/lora`文件夹并安装 `lora` 对话脚本的要求：移动到 lora 文件夹并安装要求。  
```  
cd mlx-examples/lora  
pip install -r requirements.txt  
```  
  
一旦安装了要求，我们就可以使用`lora.py`脚本来启动训练。  
  
有几个参数需要了解。你可以运行带有`--help`参数的脚本来了解更多信息，或者查看 [Github 上的文档](https://github.com/ml-explore/mlx-examples/tree/main/lora#run)。  
  
这是我用来训练和开始微调的确切命令：启动微调过程的命令。  
```  
python lora.py \  
  --train \  
  --model mistralai/Mistral-7B-Instruct-v0.2 \  
  --data ~/Developer/AI/Models/fine-tuning/data \  
  --batch-size 2 \  
  --lora-layers 8 \  
  --iters 1000  
```  
  
该`--model`参数应指向[Hugging Face 上模型的路径](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2)。如果您有本地模型，则可以提供本地 MLX 转换模型的模型目录。有关模型转换过程的更多信息，请查看[转换文档](https://github.com/ml-explore/mlx-examples/tree/main/lora#setup)。  
  
该`--data`参数是可选的，应该指向包含您的`train.jsonl`和`valid.jsonl`文件的文件夹，默认情况下它将在当前目录中查找。  
  
您需要尝试使用`batch-size`和`lora-layers`参数。一般来说，系统内存越大，这些数字就可以越高。文档对此有一些[提示](https://github.com/ml-explore/mlx-examples/tree/main/lora#memory-issues)。  
  
这里的参数`--iters`并非绝对必要，默认值为 1000 次迭代。我之所以包含它，是因为您可能仍想尝试一下。  
  
### 步骤 4：坐下来，听听电脑风扇狂暴的声音  
  
一旦你启动了训练过程，它会给你关于你的训练、验证损失、令牌和每秒迭代次数的有用反馈。  
  
使用我的 M1 Max 和 32GB 系统内存、批处理大小 1 和 LoRA 层 4，我平均可以得到大约 [260 个令牌/秒](https://twitter.com/apeatling/status/1742986333180821538)。使用上述设置，我最终平均得到接近 120 个令牌/秒：随着时间的推移，模型训练输出。  
```  
Loading pretrained model  
Total parameters 7242.584M  
Trainable parameters 0.852M  
Loading datasets  
Training  
Iter 1: Val loss 0.868, Val took 94.857s  
Iter 10: Train loss 0.793, It/sec 0.223, Tokens/sec 229.758  
Iter 20: Train loss 0.683, It/sec 0.293, Tokens/sec 218.803  
Iter 30: Train loss 0.768, It/sec 0.072, Tokens/sec 94.586  
Iter 40: Train loss 0.686, It/sec 0.081, Tokens/sec 78.442  
Iter 50: Train loss 0.734, It/sec 0.230, Tokens/sec 191.031  
Iter 60: Train loss 0.600, It/sec 0.043, Tokens/sec 85.585  
Iter 70: Train loss 0.691, It/sec 0.027, Tokens/sec 33.718  
Iter 80: Train loss 0.568, It/sec 0.031, Tokens/sec 46.508  
  
[...]  
```  
  
我也尝试在具有 16GB 系统内存的 M1 MacBook Air 上进行训练。它显然速度较慢，但如果和的值较低`batch-size`且`lora-layers`没有其他程序运行，则应该没问题。您还可以尝试使用 4 位量化来减少内存需求。  
  
根据基准测试，具有更多内存的新系统将运行得[更快](https://github.com/TristanBilot/mlx-benchmark)。  
  
### 步骤 5：找到您的 adapters.npz  
微调完成后，您会在开始训练的文件夹中找到一个文件`adapters.npz`。此文件包含微调后对基础模型进行的额外权重更改，您将在最后一步中提供此文件的位置：[测试和与微调后的 LLM 交互](https://apeatling.com/articles/part-4-testing-and-interacting-with-your-fine-tuned-llm/)。  
  
  
## 第 4 部分：测试并与经过微调的 LLM 进行交互  
### 步骤 1：确定提示以比较微调结果  
为了测试并查看微调模型的结果，您需要将微调模型与基础模型进行比较。  
  
我不会详细介绍实现此目的的最佳方法，您可能希望为试图通过微调来改进的输出创建一些提示。  
  
根据您进行的微调类型，您可以采用定量或定性的方法来评估结果。  
  
### 步骤 2：在基础模型和微调模型上测试你的提示  
MLX 库具有一些内置功能来处理模型推理。  
  
要启动基础模型，请前往`repollms`内的文件夹`mlx-examples`。您可以使用以下`mlx_lm.generate`命令启动基础模型：使用未微调的基础模型的命令。  
```  
python -m mlx_lm.generate --model mistralai/Mistral-7B-Instruct-v0.2 --max-tokens 1000 --prompt "How do I build an online store using WordPress that will allow me to sell car parts? I want to be able to ship them and charge people using credit cards."  
```  
  
该`--model`参数应指向[Hugging Face 上模型的路径](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2)。如果您有本地模型，则可以提供本地 MLX 转换模型的模型目录。有关模型转换过程的更多信息，请查看[转换文档](https://github.com/ml-explore/mlx-examples/tree/main/lora#setup)。  
  
为了提示您的微调模型，您需要提供适配器文件的位置。您可以从`repolora`中的文件夹中执行此操作`mlx-examples`。使用`lora.py`脚本和以下参数：使用您微调模型的命令。  
```  
python lora.py  --model mistralai/Mistral-7B-Instruct-v0.2 --adapter-file ./fine-tuning/adapters/adapters.npz  --num-tokens 1000  --prompt "How do I build an online store using WordPress that will allow me to sell car parts? I want to be able to ship them and charge people using credit cards."  
```  
  
该`--model`参数应指向[Hugging Face 上模型的路径](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2)。如果您有本地模型，则可以提供本地 MLX 转换模型的模型目录。有关模型转换过程的更多信息，请查看[转换文档](https://github.com/ml-explore/mlx-examples/tree/main/lora#setup)。  
  
在上述两个命令中，请确保将`--model`和`--adapter-file`参数替换为适合您设置的正确的 `Hugging Face` 路径(或本地模型路径)和适配器文件位置。    
  
该`num-tokens`参数是可选的，默认为 1000。根据您预期的答案长度，您可能需要对其进行调整。  
  
### 步骤 3：分析结果  
我对 Mistral 7B 的 WordPress 微调只是一个有趣的练习，所以我没有准备对结果进行非常彻底的分析。  
  
我尝试了 10 到 20 个不同的提示，其中包含更复杂的 WordPress 问题，并评估了与基本模型相比答案的详尽程度和完整性。  
  
正如我上面提到的，根据您想要的结果类型，您可能对如何评估有具体的想法。这取决于你。  
  
为了完整起见，以下是[基本模型的提示输出](https://raw.githubusercontent.com/apeatling/simple-guide-to-mlx-finetuning/trunk/results-example/before.txt)，以及上述示例中使用的提示的[微调模型](https://raw.githubusercontent.com/apeatling/simple-guide-to-mlx-finetuning/trunk/results-example/after.txt)。您可以看到答案的深度和质量已显著提高，但响应长度也大幅增加。还需要做更多的工作！  
  
### 步骤 4：将适配器融合到新模型中  
如果您对新的微调模型感到满意，您可以继续将 LoRA 适配器文件融合回基础模型。  
  
这样做可以更轻松地使用传统推理工具运行模型（ [尽管GGUF 格式尚未完全准备好](https://github.com/ml-explore/mlx/pull/350) ）。如果您愿意，您还可以将经过微调的模型上传回[Hugging Face MLX 社区](https://huggingface.co/mlx-community)。  
  
要融合您的模型，请使用文件夹中的以下内容lora：将适配器与基本模型融合的命令。  
```  
python fuse.py --model mistralai/Mistral-7B-Instruct-v0.2 --adapter-file ./fine-tuning/adapters/adapters.npz --save-path ./Models/My-Mistral-7B-fine-tuned  --upload-name My-Mistral-7B-fine-tuned  --de-quantize   
```  
  
该`--model`参数应指向[Hugging Face 上模型的路径](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2)。如果您有本地模型，则可以提供本地 MLX 转换模型的模型目录。有关模型转换过程的更多信息，请查看[转换文档](https://github.com/ml-explore/mlx-examples/tree/main/lora#setup)。   
  
如果您的适配器文件位于同一目录中，并且您想要将新模型保存到同一目录，则`adapter-file`和`save-path`参数是可选的。  
  
如果您不想将微调后的模型上传至 Hugging Face，您可以省略该`upload-name`参数，它将仅保存到您的`save-path位`置（如果您省略此参数，则保存到同一文件夹）。    
  
最后，如果您使用本地基础 MLX 转换模型而不是Hugging Face的模型，请提供文件夹位置作为`model`参数。在这种情况下，您需要在`hf-path`参数中提供原始 Hugging Face 模型路径，以将微调后的版本上传到 Hugging Face。  
  
下一步，我们将把您的微调模型转换为 GGUF 格式。为此，我们必须使用参数进行反量化`--de-quantize`。如果您不想转换为 GGUF，则可以省略此参数。  
  
  
### 如何与我的微调模型进行持续对话 - 发布我的模型GGUF格式给别人使用    
使用 MLX 示例库中的工具进行提示(prompt)是一回事，但最明显的问题是“如何与我的微调模型进行持续对话”？    
  
答案是将其转换为 GGUF 格式。虽然 `mlx-examples` repo 没有内置方法来执行此操作，但您可以在上一步中转换融合的微调模型。  
  
为此，您需要按照[使用 llama.cpp 转换为 GGUF](https://www.secondstate.io/articles/convert-pytorch-to-gguf/) 的说明进行操作。一旦您拥有模型的 GGUF 版本，您就可以在 Ollama、LM Studio 和其他应用中运行推理。  
  
## 参考
https://docs.mistral.ai/guides/finetuning/  
  
https://github.com/mistralai/mistral-finetune  
    
  
#### [期望 PostgreSQL|开源PolarDB 增加什么功能?](https://github.com/digoal/blog/issues/76 "269ac3d1c492e938c0191101c7238216")
  
  
#### [PolarDB 开源数据库](https://openpolardb.com/home "57258f76c37864c6e6d23383d05714ea")
  
  
#### [PolarDB 学习图谱](https://www.aliyun.com/database/openpolardb/activity "8642f60e04ed0c814bf9cb9677976bd4")
  
  
#### [购买PolarDB云服务折扣活动进行中, 55元起](https://www.aliyun.com/activity/new/polardb-yunparter?userCode=bsb3t4al "e0495c413bedacabb75ff1e880be465a")
  
  
#### [PostgreSQL 解决方案集合](../201706/20170601_02.md "40cff096e9ed7122c512b35d8561d9c8")
  
  
#### [德哥 / digoal's Github - 公益是一辈子的事.](https://github.com/digoal/blog/blob/master/README.md "22709685feb7cab07d30f30387f0a9ae")
  
  
#### [About 德哥](https://github.com/digoal/blog/blob/master/me/readme.md "a37735981e7704886ffd590565582dd0")
  
  
![digoal's wechat](../pic/digoal_weixin.jpg "f7ad92eeba24523fd47a6e1a0e691b59")
  
