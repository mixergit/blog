## 提升RAG召回效果的方法探讨    
                                                                        
### 作者                                            
digoal                                            
                                                   
### 日期                                                 
2024-08-23                                            
                                                
### 标签                                              
PostgreSQL , PolarDB , DuckDB , RAG , 召回 , 大模型          
                                                                       
----                                                
                                                              
## 背景    
看完mistral的RAG章节, 思考了一下提升RAG召回效果的方法.  
  
https://docs.mistral.ai/guides/rag/     
  
这是之前写的关于RAG的实践类文章, 召回效果是个大课题    
- [《AI大模型+全文检索+tf-idf+向量数据库+我的文章 系列之2 - 我的数字人: 4000余篇github blog文章投喂大模型中》](../202407/20240719_01.md)    
- [《AI大模型+全文检索+tf-idf+向量数据库+我的文章 系列之3 - 微调后, 我的数字人变聪明了》](../202407/20240722_01.md)    
- [《AI大模型+全文检索+tf-idf+向量数据库+我的文章 系列之4 - RAG 自动提示微调(prompt tuning)》](../202407/20240723_01.md)    
  
RAG的目的是让大模型依据问题相关的文章来进行回答, 从而得到更精准的回复, 或者回复大模型没有学习过的私有领域知识. 所以RAG的效果取决于文章本身的质量, 以及如何检索到高质量的文章.  
  
如果仅仅单一依据 “问题向量 <-> 文章向量/标题向量/总结向量 相似” 或 “问题关键词全文检索” 或 “关键词” 或 “bm25” 来检索, 往往无法得到高质量文章.    
  
## 提升RAG召回效果的方法探讨  
我拆成了几个步骤, 作为方法论, 大家可以一起来探讨.  
  
1、使用大模型 OR 机器学习(训练所有文章算出tf-idf, 使用全文检索拆词按tf-idf算法排名得到关键词, 我的github有相应文章) 对私有知识库相关文章进行提取: 分类/关键词/总结     
  
2、分析文章的知识关联图谱, 得到每篇文章的reference 文章(s), 分析每篇关联文章得到对应的关联关键词(s)      
  
3、将文章拆分为 chunk text(s)     
  
How to split:   
  
While the simplest method is to split the text by character, there are other options depending on the use case and document structure. For example, to avoid exceeding token limits in API calls, it may be necessary to split the text by tokens. To maintain the cohesiveness of the chunks, it can be useful to split the text by sentences, paragraphs, or HTML headers. If working with code, it's often recommended to split by meaningful code chunks for example using an Abstract Syntax Tree (AST) parser.  
  
4、使用大模型 OR 机器学习(训练所有文章算出tf-idf, 使用全文检索拆词按tf-idf算法排名得到关键词) 对chunk text(s)进行提取: 关键词    
  
5、使用统一的embedding大模型生成各文本相应向量     
  
6、将数据存入数据库, 信息如下:   
```  
  文章id int,   
  文章分类 text,   
  文章分类 vector,   
  文章关键词 text数组,   
  文章关键词 vector,   
  文章总结 text,   
  文章总结 vector,   
  reference文章id int数组,   
  reference文章关联关键词 text数组,   
  reference文章关联关键词 vector,   
  chunk id int,  -- 同一篇文章的chunk按内容顺序分配ID   
  chunk text,   
  chunk vector,     
  chunk关键词 text数组,   
  chunk关键词 vector    
```  
  
7、用户发起问题请求      
  
8、先使用大模型模型生成虚拟回答(解答, 或对问题进行分析分解得到需要的知识点)     
  
9、使用统一的embedding大模型生成“问题请求和虚拟回答”向量      
  
Create embeddings for a question  
  
Whenever users ask a question, we also need to create embeddings for this question using the same embedding models as before.  
```  
question = "What were the two main things the author worked on before college?"  
question_embeddings = np.array([get_text_embedding(question)])  
```  
  
Considerations:  
  
Hypothetical Document Embeddings (HyDE): In some cases, the user's question might not be the most relevant query to use for identifying the relevant context. Instead, it maybe more effective to generate a hypothetical answer or a hypothetical document based on the user's query and use the embeddings of the generated text to retrieve similar text chunks.  
  
10、结合问题请求和虚拟回答, 使用大模型 OR 机器学习(训练所有文章算出tf-idf, 使用全文检索拆词按tf-idf算法排名得到关键词) 提取关键词      
  
11、使用统一的embedding大模型生成“关键词”向量      
  
12、召回相关文章:    
  
精确性从高到低的方法顺序进行召回 (优中选优)  你也可以试试反过来?     
  
12\.1、使用(bm25, 全文检索等技术, 我的github有相应文章)对关键词进行第一道召回    
  
Considerations:  
  
Retrieval methods: There are a lot different retrieval strategies. In our example, we are showing a simple similarity search with embeddings. Sometimes when there is metadata available for the data, it's better to filter the data based on the metadata first before performing similarity search. There are also other statistical retrieval methods like TF-IDF and BM25 that use frequency and distribution of terms in the document to identify relevant text chunks.  
  
得到一批文章id, 作为下一步的范围   
  
12\.2、reference文章关联关键词 vector <-> “关键词”向量   
  
Retrieve similar chunks from the vector database  
  
We can perform a search on the vector database with index.search, which takes two arguments: the first is the vector of the question embeddings, and the second is the number of similar vectors to retrieve. This function returns the distances and the indices of the most similar vectors to the question vector in the vector database. Then based on the returned indices, we can retrieve the actual relevant text chunks that correspond to those indices.  
```  
D, I = index.search(question_embeddings, k=2) # distance, index  
retrieved_chunk = [chunks[i] for i in I.tolist()[0]]  
```  
  
得到一批reference文章id   
  
上面两步的文章id+reference文章id, 作为下一步的范围   
  
12\.3、chunk关键词 vector <-> “关键词”向量  
  
得到一批文章id, 作为下一步的范围    
  
12\.4、文章关键词 vector <-> “关键词”向量  
  
得到一批文章id    
  
12\.5、最后排序选择多少篇文章输入作为prompt的context?  取决于大模型的上下文长度限制(128k, ... ?)     
  
虽然是chunk等过滤条件进行搜索, 但要以整篇文章为内容进行召回, 以免丢失上下文.  
  
Retrieved document: Do we always retrieve individual text chunk as it is? Not always.   
  
Sometime, we would like to include more context around the actual retrieved text chunk. We call the actual retrieved text chunk "child chunk" and our goal is to retrieve a larger "parent chunk" that the "child chunk" belongs to.  
  
One common issue in the retrieval process is the "lost in the middle" problem where the information in the middle of a long context gets lost. Our models have tried to mitigate this issue. For example, in the passkey task, our models have demonstrated the ability to find a "needle in a haystack" by retrieving a randomly inserted passkey within a long prompt, up to 32k context length. However, it is worth considering experimenting with reordering the document to determine if placing the most relevant chunks at the beginning and end leads to improved results.  
  
文档的新鲜度也是考虑排序的维度之一:  
  
On occasion, we might also want to provide weights to our retrieve documents. For example, a time-weighted approach would help us retrieve the most recent document.  
   
第12步可以关注一下paradedb的理念, 结合全文检索+语义搜索的混合检索技术.    
- https://www.paradedb.com/blog/elasticsearch_vs_postgres
- https://github.com/paradedb/paradedb/tree/dev/pg_search
- https://docs.paradedb.com/api-reference/full-text/filtering
- https://docs.paradedb.com/welcome/quickstart
   
13、合并问题和参考信息.   
```
prompt = f"""
Context information is below.
---------------------
{retrieved_chunk}
---------------------
Given the context information and not prior knowledge, answer the query.
Query: {question}
Answer:
"""
```
   
#### [期望 PostgreSQL|开源PolarDB 增加什么功能?](https://github.com/digoal/blog/issues/76 "269ac3d1c492e938c0191101c7238216")
  
  
#### [PolarDB 开源数据库](https://openpolardb.com/home "57258f76c37864c6e6d23383d05714ea")
  
  
#### [PolarDB 学习图谱](https://www.aliyun.com/database/openpolardb/activity "8642f60e04ed0c814bf9cb9677976bd4")
  
  
#### [购买PolarDB云服务折扣活动进行中, 55元起](https://www.aliyun.com/activity/new/polardb-yunparter?userCode=bsb3t4al "e0495c413bedacabb75ff1e880be465a")
  
  
#### [PostgreSQL 解决方案集合](../201706/20170601_02.md "40cff096e9ed7122c512b35d8561d9c8")
  
  
#### [德哥 / digoal's Github - 公益是一辈子的事.](https://github.com/digoal/blog/blob/master/README.md "22709685feb7cab07d30f30387f0a9ae")
  
  
#### [About 德哥](https://github.com/digoal/blog/blob/master/me/readme.md "a37735981e7704886ffd590565582dd0")
  
  
![digoal's wechat](../pic/digoal_weixin.jpg "f7ad92eeba24523fd47a6e1a0e691b59")
  
