## DeepSeek多模态大模型Janus的使用 (文本生成 , 图像生成 , 图像分析)   
                                                                                       
### 作者                                                                
digoal                                                                       
                                                                    
### 日期                                                                                 
2025-02-08                                                                      
                                                                                
### 标签                                                                              
PostgreSQL , PolarDB , DuckDB , 多模态大模型 , DeepSeek , Janus , 文本生成 , 图像生成 , 图像分析    
                                                                                         
----                                                                     
                                                                             
## 背景     
  
DeepSeek发布的Janus-Pro-7B是一款开源多模态大模型，支持图像识别/分析、基于文本生成图像，性能超越OpenAI的DALL·E 3。     
  
以下是 DeepSeek Janus Pro-7B 如何在本地部署的详细步骤、技巧及注意事项   
  
机器  
```  
macbook m2 16G  
```  
  
python版本: 因为这篇文章, python版本升级到了3.12.8 [《mac + mlx-examples 大模型微调(fine-tuning)实践 - 让它学完所有PolarDB文章》](../202501/20250108_01.md)      
```  
$ python -V  
Python 3.12.8  
```  
  
如果你的系统没有安装 pkg-config 的话, 需要先安装一下, 否则安装 requirements.txt 里的 sentencepiece 时将报错.  
  
https://pkg-config.freedesktop.org/releases/  
  
```  
curl https://pkg-config.freedesktop.org/releases/pkg-config-0.29.2.tar.gz -o ./pkg-config-0.29.2.tar.gz  
tar -zxvf pkg-config-0.29.2.tar.gz  
cd pkg-config-0.29.2  
./configure  --with-internal-glib  
make  
sudo make install  
```  
  
克隆 Janus  
```  
git clone --depth 1 https://github.com/deepseek-ai/Janus.git  
cd Janus  
```  
  
修改 requirements.txt , 因为`Python 3.12.8` 无法安装 `torch==2.0.1` 版本 , 改成`3.12`最低允许的`2.2` , 或者你可以考虑将`Python` 回退到 `3.8` .    
  
https://pypi.org/project/torch/#history  
  
```  
vi requirements.txt  
  
torch==2.2.2  
```  
  
安装依赖包  
```  
cd Janus    
pip install -r requirements.txt  
pip install -e .
pip install setuptools
```  
  
  
<b> macOS 不需要安装注意力集中插件flash-attention, 仅支持Nivdia CUDA: </b>    
```   
cd ~  
git clone --depth 1 https://github.com/HazyResearch/flash-attention.git  
cd flash-attention  
python setup.py install  
```  
  
下载Janus-Pro-7B模型  
  
方法一:  
```  
cd Janus  
HF_ENDPOINT=https://hf-mirror.com nohup huggingface-cli download deepseek-ai/Janus-Pro-7B --local-dir ./models/Janus-Pro-7B --resume-download --cache-dir ./cache --workers 8 >/dev/null 2>&1 &  
```  
  
方法二:   
  
手动从 HuggingFace 仓库 下载模型文件(所有文件)   
  
https://huggingface.co/deepseek-ai/Janus-Pro-7B/tree/main    
  
将下载好的模型文件(所有文件) 移动至 Janus/models/Janus-Pro-7B 目录   
```  
cd Janus   
mkdir -p models/Janus-Pro-7B  
```  
  
```  
$ ll  
-rw-r--r--@  1 digoal  staff   9.3G Feb  8 17:15 pytorch_model-00001-of-00002.bin  
-rw-r--r--@  1 digoal  staff   4.5G Feb  8 17:37 pytorch_model-00002-of-00002.bin  
-rw-r--r--@  1 digoal  staff   1.3K Feb  8 17:38 config.json  
-rw-r--r--@  1 digoal  staff    96K Feb  8 17:38 janus_pro_teaser1.png  
-rw-r--r--@  1 digoal  staff   518K Feb  8 17:38 janus_pro_teaser2.png  
-rw-r--r--@  1 digoal  staff   346B Feb  8 17:38 preprocessor_config.json  
-rw-r--r--@  1 digoal  staff   210B Feb  8 17:38 processor_config.json  
-rw-r--r--@  1 digoal  staff    87K Feb  8 17:38 pytorch_model.bin.index.json  
-rw-r--r--@  1 digoal  staff   344B Feb  8 17:38 special_tokens_map.json  
-rw-r--r--@  1 digoal  staff   4.5M Feb  8 17:38 tokenizer.json  
-rw-r--r--@  1 digoal  staff   285B Feb  8 17:38 tokenizer_config.json  
```  
  
修改demo文件中使用模型的地址, 修改后不需要每次都通过`--model-path`指定模型位置.    
```  
cd Janus   
  
vi demo/app_januspro.py  
  
# Load model and processor  
# model_path = "deepseek-ai/Janus-Pro-7B"  
model_path = "/Users/digoal/Janus/models/Janus-Pro-7B"  
  
  
######  
  
vi demo/fastapi_app.py   
  
# Load model and processor  
# model_path = "deepseek-ai/Janus-1.3B"  
model_path = "/Users/digoal/Janus/models/Janus-Pro-7B"  
```  
  
其他优化? 使用 apple chip mps?  <b> 可能已自动支持mps, 参考文末说明. </b>    
  
该文章可能是AI生成的, ollama的API根本不支持文生图, 而且ollama也没有`deepseek-ai/janus-pro-7b`这个镜像.   
- https://ghost.codersera.com/blog/running-deepseek-janus-pro-7b-on-mac-using-comfyui/    
  
```  
Enable Metal Performance Shaders (MPS) for GPU acceleration:  
# In your Python script or ComfyUI config:    
import torch    
torch.device("mps")  # Prioritize Metal GPU over CPU    
  
cd Janus   
  
$ grep -r cpu *|grep torch  
README.md:    dec = dec.to(torch.float32).cpu().numpy().transpose(0, 2, 3, 1)  
README.md:    dec = dec.to(torch.float32).cpu().numpy().transpose(0, 2, 3, 1)  
demo/app_janusflow.py:cuda_device = 'cuda' if torch.cuda.is_available() else 'cpu'  
demo/app_janusflow.py:    dec = dec.to(torch.float32).cpu().numpy().transpose(0, 2, 3, 1)  
demo/app_januspro.py:cuda_device = 'cuda' if torch.cuda.is_available() else 'cpu'  
demo/app_januspro.py:    dec = dec.to(torch.float32).cpu().numpy().transpose(0, 2, 3, 1)  
demo/app.py:cuda_device = 'cuda' if torch.cuda.is_available() else 'cpu'  
demo/app.py:    dec = dec.to(torch.float32).cpu().numpy().transpose(0, 2, 3, 1)  
demo/fastapi_app.py:cuda_device = 'cuda' if torch.cuda.is_available() else 'cpu'  
demo/fastapi_app.py:    dec = dec.to(torch.float32).cpu().numpy().transpose(0, 2, 3, 1)  
generation_inference.py:    dec = dec.to(torch.float32).cpu().numpy().transpose(0, 2, 3, 1)  
interactivechat.py:    dec = dec.to(torch.float32).cpu().numpy().transpose(0, 2, 3, 1)  
janus/models/siglip_vit.py:        state_dict = torch.load(ckpt_path, map_location="cpu")  
janus/janusflow/models/siglip_vit.py:        state_dict = torch.load(ckpt_path, map_location="cpu")  
janus.egg-info/PKG-INFO:    dec = dec.to(torch.float32).cpu().numpy().transpose(0, 2, 3, 1)  
janus.egg-info/PKG-INFO:    dec = dec.to(torch.float32).cpu().numpy().transpose(0, 2, 3, 1)  
  
  
$ grep -r -i cuda *|grep devi  
demo/app_janusflow.py:cuda_device = 'cuda' if torch.cuda.is_available() else 'cpu'  
demo/app_janusflow.py:vl_gpt = vl_gpt.to(torch.bfloat16).to(cuda_device).eval()  
demo/app_janusflow.py:vae = vae.to(torch.bfloat16).to(cuda_device).eval()  
demo/app_janusflow.py:    ).to(cuda_device, dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float16)  
demo/app_januspro.py:cuda_device = 'cuda' if torch.cuda.is_available() else 'cpu'  
demo/app_januspro.py:    ).to(cuda_device, dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float16)  
demo/app_januspro.py:    tokens = torch.zeros((parallel_size * 2, len(input_ids)), dtype=torch.int).to(cuda_device)  
demo/app_januspro.py:    generated_tokens = torch.zeros((parallel_size, image_token_num_per_image), dtype=torch.int).to(cuda_device)  
demo/app.py:cuda_device = 'cuda' if torch.cuda.is_available() else 'cpu'  
demo/app.py:    ).to(cuda_device, dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float16)  
demo/app.py:    tokens = torch.zeros((parallel_size * 2, len(input_ids)), dtype=torch.int).to(cuda_device)  
demo/app.py:    generated_tokens = torch.zeros((parallel_size, image_token_num_per_image), dtype=torch.int).to(cuda_device)  
demo/fastapi_app.py:cuda_device = 'cuda' if torch.cuda.is_available() else 'cpu'  
demo/fastapi_app.py:    ).to(cuda_device, dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float16)  
demo/fastapi_app.py:    tokens = torch.zeros((parallel_size * 2, len(input_ids)), dtype=torch.int).to(cuda_device)  
demo/fastapi_app.py:    generated_tokens = torch.zeros((parallel_size, image_token_num_per_image), dtype=torch.int).to(cuda_device)  
```  
  
  
启动应用, 参考: https://github.com/deepseek-ai/Janus/blob/main/demo/Janus_colab_demo.ipynb        
  
命令行交互界面：    
```  
cd Janus  
  
python demo/app_januspro.py    
```  
  
WEB多模态界面（支持图像生成与问答）：      
```  
cd Janus  
  
python demo/fastapi_app.py   
```  
  
访问 `http://localhost:7860` 或 `http://127.0.0.1:7860` 使用WEB交互界面     
  
启动WEB多模态界面日志如下:   
```
$ python demo/app_januspro.py    

Python version is above 3.10, patching the collections module.
/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/transformers/models/auto/image_processing_auto.py:590: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead
  warnings.warn(
Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.48, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
Some kwargs in processor config are unused and will not have any effect: num_image_tokens, add_special_token, sft_format, image_tag, mask_prompt, ignore_id. 
Running on local URL:  http://127.0.0.1:7860
IMPORTANT: You are using gradio version 3.48.0, however version 4.44.1 is available, please upgrade.
--------

Could not create share link. Missing file: /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/gradio/frpc_darwin_arm64_v0.2. 

Please check your internet connection. This can happen if your antivirus software blocks the download of this file. You can install manually by following these steps: 

1. Download this file: https://cdn-media.huggingface.co/frpc-gradio-0.2/frpc_darwin_arm64
2. Rename the downloaded file to: frpc_darwin_arm64_v0.2
3. Move the file to this location: /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/gradio
```
  
解决以上问题
```
download https://cdn-media.huggingface.co/frpc-gradio-0.2/frpc_darwin_arm64

mv frpc_darwin_arm64 frpc_darwin_arm64_v0.2
mv frpc_darwin_arm64_v0.2 /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/gradio/
```
   
重新启动  
```
$ python demo/app_januspro.py  

...
Could not create share link. Please check your internet connection or our status page: https://status.gradio.app.
这个问题不影响, 可以打开 http://127.0.0.1:7860 
```
  
但是有个问题, 生成图片时不能使用apple chip GPU, 有待改进.  试试:    
- [《AI生成图片: DiffusionBee/ComfyUI/Diffusers 如何使用stable diffusion在Apple Silicon芯片的Mac上画图（GPU加速）- 文生图text-to-image》](../202411/20241115_02.md)  
- [《用tcpdump抓包了解程序到底在访问什么URL, 科学上网/翻墙场景 - 搞定DiffusionBee下载文生图模型》](../202411/20241116_01.md)  
  
或者试试comfyui+diffusion
- https://comfyui-wiki.com/zh/install/install-comfyui/install-comfyui-on-mac
- https://huggingface.co/stabilityai/stable-diffusion-3.5-large-turbo/tree/main
- https://huggingface.co/spaces/stabilityai/stable-diffusion-3.5-large-turbo
  
  
  
  
内16G 存爆掉? 可以下载1B的试试.  
  
https://huggingface.co/deepseek-ai/Janus-Pro-1B/tree/main  
  
  
## 参考  
https://huggingface.co/spaces/deepseek-ai/Janus-Pro-7B  
  
https://github.com/deepseek-ai/Janus  
  
https://www.yjpoo.com/article/5101.html  
  
https://huggingface.co/deepseek-ai/Janus-Pro-7B  
  
DEMO:  
- https://github.com/deepseek-ai/Janus/blob/main/demo/Janus_colab_demo.ipynb   
  
以下文章可能是AI生成的, ollama的API根本不支持文生图, 而且ollama也没有`deepseek-ai/janus-pro-7b`这个镜像.   
- https://ghost.codersera.com/blog/running-deepseek-janus-pro-7b-on-mac-using-comfyui/    
  
https://ollama.com/erwan2/DeepSeek-Janus-Pro-7B  
  
https://developer.apple.com/metal/pytorch/  
```  
# 测试是否支持mps?   
  
$ vi p.py  
  
import torch  
if torch.backends.mps.is_available():  
    mps_device = torch.device("mps")  
    x = torch.ones(1, device=mps_device)  
    print (x)  
else:  
    print ("MPS device not found.")  
  
$ python p.py  
tensor([1.], device='mps:0')  
```  
    
  
#### [期望 PostgreSQL|开源PolarDB 增加什么功能?](https://github.com/digoal/blog/issues/76 "269ac3d1c492e938c0191101c7238216")
  
  
#### [PolarDB 开源数据库](https://openpolardb.com/home "57258f76c37864c6e6d23383d05714ea")
  
  
#### [PolarDB 学习图谱](https://www.aliyun.com/database/openpolardb/activity "8642f60e04ed0c814bf9cb9677976bd4")
  
  
#### [PostgreSQL 解决方案集合](../201706/20170601_02.md "40cff096e9ed7122c512b35d8561d9c8")
  
  
#### [德哥 / digoal's Github - 公益是一辈子的事.](https://github.com/digoal/blog/blob/master/README.md "22709685feb7cab07d30f30387f0a9ae")
  
  
#### [About 德哥](https://github.com/digoal/blog/blob/master/me/readme.md "a37735981e7704886ffd590565582dd0")
  
  
![digoal's wechat](../pic/digoal_weixin.jpg "f7ad92eeba24523fd47a6e1a0e691b59")
  
