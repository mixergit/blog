## PostgreSQL 18 preview - 增加fast-path lock slots, 提升访问多对象的高并发OLTP业务性能    
                                                              
### 作者                                  
digoal                                  
                                         
### 日期                                       
2024-09-24                                  
                                      
### 标签                                    
PostgreSQL , PolarDB , DuckDB , fast-path lock slots , 锁冲突 , 高并发 , 小事务 , 多核系统 , OLTP            
                                                             
----                                      
                                                    
## 背景    
增加fast-path lock slots, 降低multi-core系统fast-path lock冲突, 提升高并发(且事务中需要访问较多对象(包括视图、索引、表分区等))小事务OLTP业务的性能.     
  
未来可能会新增GUC来配置fast-path lock slots. 目前还是通过max_locks_per_transaction计算得到.  
  
https://git.postgresql.org/gitweb/?p=postgresql.git;a=commit;h=c4d5cb71d229095a39fda1121a75ee40e6069a2a  
```  
Increase the number of fast-path lock slots  
author	Tomas Vondra <tomas.vondra@postgresql.org>	  
Sat, 21 Sep 2024 18:06:49 +0000 (20:06 +0200)  
committer	Tomas Vondra <tomas.vondra@postgresql.org>	  
Sat, 21 Sep 2024 18:09:35 +0000 (20:09 +0200)  
commit	c4d5cb71d229095a39fda1121a75ee40e6069a2a  
tree	08116cc60379809723de39c6a1a8b9d80325963e	tree  
parent	b524974106acb05ae4f9c2178153c3ead72eaf04	commit | diff  
  
Increase the number of fast-path lock slots   
  
Replace the fixed-size array of fast-path locks with arrays, sized on  
startup based on max_locks_per_transaction. This allows using fast-path  
locking for workloads that need more locks.  
  
The fast-path locking introduced in 9.2 allowed each backend to acquire  
a small number (16) of weak relation locks cheaply. If a backend needs  
to hold more locks, it has to insert them into the shared lock table.  
This is considerably more expensive, and may be subject to contention  
(especially on many-core systems).  
  
The limit of 16 fast-path locks was always rather low, because we have  
to lock all relations - not just tables, but also indexes, views, etc.  
For planning we need to lock all relations that might be used in the  
plan, not just those that actually get used in the final plan. So even  
with rather simple queries and schemas, we often need significantly more  
than 16 locks.  
  
As partitioning gets used more widely, and the number of partitions  
increases, this limit is trivial to hit. Complex queries may easily use  
hundreds or even thousands of locks. For workloads doing a lot of I/O  
this is not noticeable, but for workloads accessing only data in RAM,  
the access to the shared lock table may be a serious issue.  
  
This commit removes the hard-coded limit of the number of fast-path  
locks. Instead, the size of the fast-path arrays is calculated at  
startup, and can be set much higher than the original 16-lock limit.  
The overall fast-path locking protocol remains unchanged.  
  
The variable-sized fast-path arrays can no longer be part of PGPROC, but  
are allocated as a separate chunk of shared memory and then references  
from the PGPROC entries.  
  
The fast-path slots are organized as a 16-way set associative cache. You  
can imagine it as a hash table of 16-slot "groups". Each relation is  
mapped to exactly one group using hash(relid), and the group is then  
processed using linear search, just like the original fast-path cache.  
With only 16 entries this is cheap, with good locality.  
  
Treating this as a simple hash table with open addressing would not be  
efficient, especially once the hash table gets almost full. The usual  
remedy is to grow the table, but we can't do that here easily. The  
access would also be more random, with worse locality.  
  
The fast-path arrays are sized using the max_locks_per_transaction GUC.  
We try to have enough capacity for the number of locks specified in the  
GUC, using the traditional 2^n formula, with an upper limit of 1024 lock  
groups (i.e. 16k locks). The default value of max_locks_per_transaction  
is 64, which means those instances will have 64 fast-path slots.  
  
The main purpose of the max_locks_per_transaction GUC is to size the  
shared lock table. It is often set to the "average" number of locks  
needed by backends, with some backends using significantly more locks.  
This should not be a major issue, however. Some backens may have to  
insert locks into the shared lock table, but there can't be too many of  
them, limiting the contention.  
  
The only solution is to increase the GUC, even if the shared lock table  
already has sufficient capacity. That is not free, especially in terms  
of memory usage (the shared lock table entries are fairly large). It  
should only happen on machines with plenty of memory, though.  
  
In the future we may consider a separate GUC for the number of fast-path  
slots, but let's try without one first.  
  
Reviewed-by: Robert Haas, Jakub Wartak  
Discussion: https://postgr.es/m/510b887e-c0ce-4a0c-a17a-2c6abb8d9a5c@enterprisedb.com  
```  
  
  
#### [期望 PostgreSQL|开源PolarDB 增加什么功能?](https://github.com/digoal/blog/issues/76 "269ac3d1c492e938c0191101c7238216")
  
  
#### [PolarDB 开源数据库](https://openpolardb.com/home "57258f76c37864c6e6d23383d05714ea")
  
  
#### [PolarDB 学习图谱](https://www.aliyun.com/database/openpolardb/activity "8642f60e04ed0c814bf9cb9677976bd4")
  
  
#### [PostgreSQL 解决方案集合](../201706/20170601_02.md "40cff096e9ed7122c512b35d8561d9c8")
  
  
#### [德哥 / digoal's Github - 公益是一辈子的事.](https://github.com/digoal/blog/blob/master/README.md "22709685feb7cab07d30f30387f0a9ae")
  
  
#### [About 德哥](https://github.com/digoal/blog/blob/master/me/readme.md "a37735981e7704886ffd590565582dd0")
  
  
![digoal's wechat](../pic/digoal_weixin.jpg "f7ad92eeba24523fd47a6e1a0e691b59")
  
