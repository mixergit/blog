## 微调大模型时 vocab.json 和 tokenizer.json 有什么用? 如何扩充中文词汇表?    
              
### 作者              
digoal              
              
### 日期              
2025-01-26              
              
### 标签              
PostgreSQL , PolarDB , DuckDB , 大模型 , 微调 , 词汇表 , 分词方法 , vocab.json , tokenizer.json   
              
----              
              
## 背景    
<b>微调大模型时 vocab.json 和 tokenizer.json 有什么用?</b>  
  
在Hugging Face的Transformers库中，`vocab.json` 和 `tokenizer.json` 文件都与模型的分词器（Tokenizer）有关，但它们的功能和作用有所不同。   
  
1. **vocab.json**:  
   - 这个文件通常用于存储**词汇表**（vocabulary），也就是模型在训练过程中学到的所有词或子词（subwords）的集合。  
   - 对于基于词汇的分词器（比如BERT的WordPiece，GPT的Byte Pair Encoding），`vocab.json`会包含词汇与其对应的ID之间的映射。  
   - `vocab.json`的内容是一个字典，将词或子词映射到一个唯一的整数ID，这些ID在模型训练时作为输入。  
  
   例如，如果你有一个词汇表包含`['hello', 'world']`，`vocab.json`可能会是：  
   ```json  
   {  
     "hello": 1,  
     "world": 2  
   }  
   ```  
  
2. **tokenizer.json**:  
   - 这个文件则包含了**分词器的配置**，包括分词器的类型、词汇表的路径、特定的编码方式（如BPE编码的合并规则），以及可能的其他配置项（比如特殊符号的映射等）。  
   - `tokenizer.json`是一个更为全面的文件，存储了分词器的所有相关信息，除了词汇表之外，还包括如何处理标记（tokens）的规则、如何分解词语、如何处理多语言输入等。  
   - 通常，它是一个更复杂的JSON结构，包含有关分词器配置的详细信息。  
  
   例如，`tokenizer.json`可能会包含如下信息：  
   ```json  
   {  
     "type": "BPE",  
     "model": ["hello", "world"],  
     "special_tokens": {  
       "unk_token": "[UNK]",  
       "pad_token": "[PAD]"  
     },  
     "vocab_size": 5000  
   }  
   ```  
  
### 主要区别：  
- `vocab.json`：主要存储词汇到ID的映射，用于词汇表本身。  
- `tokenizer.json`：存储整个分词器的配置信息，包括词汇表、编码方式等。  
  
在实际使用中，如果你加载预训练模型的分词器（比如BERT或GPT），通常`tokenizer.json`会包含更多的信息，而`vocab.json`则是词汇表的一部分。  
  
---  
  
<b>为什么有tokenizer.json还需要vocab.json?</b>  
  
`tokenizer.json` 和 `vocab.json` 可能会同时存在的原因是它们的功能和存储的信息不同，且有些模型的实现方式需要这两者的配合。具体来说，以下是这两者的作用以及它们为何都需要存在的原因：  
  
### 1. **vocab.json 主要用于词汇表映射**  
   - **`vocab.json`** 存储的是分词器的**词汇表**，即一个词或子词（subword）到数字ID的映射。这对于分词器的基本工作至关重要，尤其是在像 BERT 或 GPT 这样的模型中，分词器需要快速查找每个词（或子词）的ID以便输入模型进行处理。  
   - 对于基于子词（subword）的方法（如BPE，WordPiece等），`vocab.json`是一个简单的映射结构，它只是记录了词汇表中每个单元（词或子词）的ID。  
     
   例如：  
   ```json  
   {  
     "hello": 1,  
     "world": 2  
   }  
   ```  
   在这个例子中，`hello` 被映射到ID `1`，`world` 被映射到ID `2`。  
  
### 2. **tokenizer.json 主要用于存储分词器的完整配置**  
   - **`tokenizer.json`** 存储的是关于分词器的**完整配置**，包括但不限于：  
     - 分词策略（如BPE、WordPiece等）  
     - 特殊符号（例如[CLS]、[SEP]、[PAD]等）  
     - 是否有其他的预处理步骤（如大小写转换）  
     - 子词的合并规则（对于BPE或SentencePiece这类方法）  
   - 它不仅包含词汇表信息，还包括如何使用词汇表的信息，例如：如何拆分单词、如何合并子词、特殊标记的处理方式等等。因此，`tokenizer.json` 是一个更复杂的配置文件，能够定义分词器的行为。  
     
   例如：  
   ```json  
   {  
     "type": "BPE",  
     "special_tokens": {  
       "unk_token": "[UNK]",  
       "pad_token": "[PAD]"  
     },  
     "vocab_size": 5000  
   }  
   ```  
  
### 3. **为什么同时需要两个文件？**  
   - **分工不同**：`vocab.json` 只关心词汇和其对应的 ID，而 `tokenizer.json` 负责存储分词器的操作细节。虽然 `tokenizer.json` 可能包含一些词汇表的信息，但它并不直接映射每个词到 ID。`vocab.json` 承担了这个任务。  
   - **灵活性**：有些模型使用的分词器可能会更复杂或具有特定的实现方式（例如，使用BPE的子词分割规则）。`tokenizer.json` 记录了这些实现细节，而 `vocab.json` 只需要关注词汇到ID的映射。  
   - **兼容性**：不同的预训练模型可能有不同的分词器实现和需求。某些模型可能只需要 `vocab.json` 来加载词汇表，而另一些模型则需要完整的分词器配置（即 `tokenizer.json`）。因此，两个文件可以为不同的使用场景提供支持。  
  
### 总结  
- **`vocab.json`** 存储的是分词器的词汇表（即词或子词与ID的映射）。  
- **`tokenizer.json`** 包含完整的分词器配置信息，包括词汇表、编码方式、特殊符号处理等详细信息。  
  
虽然有些情况下 `tokenizer.json` 可能包含部分词汇表信息，但通常它们各自有不同的功能，并且一起使用以确保分词器能够正确工作。  
  
---  
  
<b>对于没有训练过中文的模型, 在使用中文语料微调模型时, 是否需要扩充vocab.json和tokenizer.json? 如果需要扩充, 那么如何扩充?</b>  
  
在使用没有训练过中文的预训练模型（例如基于英语的大型语言模型）进行中文语料微调时，确实可能需要扩充 `vocab.json` 和 `tokenizer.json`，特别是当模型未包含足够的中文词汇时。这是因为中文和英语在词汇、字符集、语法结构上有很大的不同。下面是需要扩充的原因及扩充的方式。  
  
### 为什么需要扩充 `vocab.json` 和 `tokenizer.json`？  
1. **词汇缺失**：  
   - 许多基于英语的模型（如BERT、GPT-2等）在训练时仅包含英语词汇，因此对于中文、日文等其他语言的词汇支持较差。如果模型的词汇表中没有中文的词汇或子词，分词器将无法有效地处理中文文本。  
  
2. **子词划分**：  
   - 基于字节对编码（BPE）或WordPiece等分词技术的模型，在训练时可能会将不常见的词拆解成更小的子词（subword）。因此，当你用中文语料微调时，很多中文词汇可能会被拆分为多个子词（如果这些子词没有出现在词汇表中），这可能会导致性能下降。  
  
3. **字符集差异**：  
   - 英语和中文的字符集不同，中文有大量的汉字，而这些汉字在模型的词汇表中可能不存在。因此，必须扩展词汇表，以便能处理中文字符。  
  
### 如何扩充 `vocab.json` 和 `tokenizer.json`？  
  
#### 1. **扩充 `vocab.json`**：  
   - 扩充 `vocab.json` 主要是将中文词汇或子词添加到现有的词汇表中。常见的做法是使用分词器（如BPE或WordPiece）从中文语料中生成新的词汇表。  
     
   **具体步骤**：  
   - **准备中文语料**：你需要准备大量的中文文本，作为新的训练语料（可以是新闻、小说、维基百科等）。  
   - **训练分词器**：使用适合的工具（如 `SentencePiece`、`tokenizers` 库等）从中文语料中训练一个新的子词分词器，并生成一个新的词汇表。  
     - 如果使用 `SentencePiece`，可以通过以下命令训练：  
       ```bash  
       spm_train --input=chinese_corpus.txt --model_type=bpe --model_prefix=chinese_model --vocab_size=8000  
       ```  
     - 这将生成一个新的 `vocab.json` 文件，包含中文子词的映射。  
  
#### 2. **扩充 `tokenizer.json`**：  
   - `tokenizer.json` 存储分词器的配置信息，包括分词器的类型、特殊符号等。扩充时，需要确保更新这个文件中的配置，以支持新的词汇表和中文分词策略。  
     
   **具体步骤**：  
   - 在训练新的分词器时，通常会生成一个 `tokenizer.json` 文件。这个文件包含了分词器的配置信息，包括词汇表、子词合并规则、特殊符号等。  
   - **更新特殊符号**：你需要确保中文相关的特殊符号（如 `[CLS]`、`[SEP]`、`[UNK]` 等）被正确配置，或者你可以根据需要自定义特殊符号。  
   - 如果使用 `tokenizers` 库来创建分词器，生成 `tokenizer.json` 文件的代码示例如下：  
     ```python  
     from tokenizers import Tokenizer, models, pre_tokenizers, decoders, trainers, processors  
       
     tokenizer = Tokenizer(models.BPE())  
     tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()  
     tokenizer.decoder = decoders.BPE()  
       
     trainer = trainers.BpeTrainer(vocab_size=8000, special_tokens=["[UNK]", "[CLS]", "[SEP]", "[PAD]"])  
     tokenizer.train(["chinese_corpus.txt"], trainer)  
     tokenizer.save("tokenizer.json")  
     ```  
  
#### 3. **微调模型时的注意事项**：  
   - **微调时加载新分词器**：微调模型时，加载的新分词器（包括更新后的 `vocab.json` 和 `tokenizer.json`）需要与模型的原始结构兼容。Hugging Face Transformers 提供了 `AutoTokenizer` 来自动加载和处理这些文件。  
     
     例如：  
     ```python  
     from transformers import AutoTokenizer, AutoModelForSequenceClassification  
  
     # 加载微调后的分词器  
     tokenizer = AutoTokenizer.from_pretrained("path_to_your_tokenizer")  
     model = AutoModelForSequenceClassification.from_pretrained("path_to_your_model")  
       
     # 使用新分词器进行文本处理  
     inputs = tokenizer("你好，世界！", return_tensors="pt")  
     ```  
  
#### 4. **使用现成的中文模型**：  
   - 如果你不希望从头开始训练分词器，可以选择使用已有的中文预训练模型（如 `bert-base-chinese` 或 `hfl/chinese-roberta-wwm-ext` 等）。这些模型已经训练了适合中文的分词器，不需要重新扩充词汇表。  
  
### 总结：  
- **需要扩充**：当原始的预训练模型未训练过中文，且其词汇表不包含中文字符或子词时，确实需要扩充 `vocab.json` 和 `tokenizer.json`。  
- **扩充方式**：通过重新训练一个适合中文的分词器，生成新的 `vocab.json` 和 `tokenizer.json`。你可以使用 `SentencePiece`、`tokenizers` 等工具从中文语料中训练新的分词器，并更新配置文件。  
- **注意**：确保在微调过程中正确加载更新后的分词器，同时检查新的词汇表是否适配中文处理需求。  
  
如果你有更具体的需求或想法，也可以进一步探讨如何实施。  
  
---  
  
<b>扩充时, 如何避免打乱已有vocab.json文件内已有词汇的映射ID? </b>  
  
在扩充现有的 `vocab.json` 时，为了避免打乱已经存在的词汇的 ID 映射，关键是要确保在添加新的词汇或子词时，新的词汇不会覆盖或改变原有词汇的 ID 映射。可以通过以下几个步骤来实现这一目标：  
  
### 扩充 `vocab.json` 时避免打乱已有映射的步骤：  
  
#### 1. **保持原有词汇的顺序和 ID**  
   - 首先，加载并保存原始的 `vocab.json` 文件，并确保将其中的词汇和 ID 映射关系保留下来。  
   - 对于新词汇，你可以将它们添加到现有词汇的末尾，确保原有的词汇 ID 保持不变。  
  
#### 2. **加载原始的 `vocab.json`**  
   - 读取现有的 `vocab.json` 文件，确保了解其中的词汇和 ID 映射。  
     
   ```python  
   import json  
  
   with open('path_to_original_vocab.json', 'r', encoding='utf-8') as f:  
       vocab = json.load(f)  
     
   # 打印现有词汇和ID映射  
   print(list(vocab.items())[:10])  # 查看前10个词和它们的ID  
   ```  
  
#### 3. **生成新的 `vocab.json`**  
   - 在原始词汇表的基础上，使用你自己的中文语料生成新的词汇。需要确保新的词汇不与现有的词汇冲突。最简单的做法是将新词汇追加到原词汇表的末尾。  
     
   - **避免冲突**：通过检查新词汇是否已经在现有的词汇表中，如果已经存在，就跳过；如果不存在，就添加到新词汇表中，并为其分配新的 ID。  
  
   ```python  
   new_vocab = {"new_word_1": len(vocab) + 1, "new_word_2": len(vocab) + 2}  # 假设这是新生成的词汇  
   vocab.update(new_vocab)  # 添加新词汇到现有词汇表  
   ```  
  
   - 生成新的 `vocab.json` 文件：  
   ```python  
   with open('new_vocab.json', 'w', encoding='utf-8') as f:  
       json.dump(vocab, f, ensure_ascii=False, indent=4)  
   ```  
  
#### 4. **生成新的分词器配置（`tokenizer.json`）**  
   - 更新完词汇表后，你还需要根据新的 `vocab.json` 重新生成 `tokenizer.json` 配置文件。这个文件记录了分词器的具体信息，例如分词规则、特殊符号等。  
     
   - 使用更新后的 `vocab.json` 来训练或更新分词器，确保新的分词器配置不会破坏原有的功能：  
   ```python  
   from tokenizers import Tokenizer, models, pre_tokenizers, decoders, trainers, processors  
  
   # 加载原始词汇表  
   tokenizer = Tokenizer(models.BPE())  
   tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()  
   tokenizer.decoder = decoders.BPE()  
  
   # 使用更新后的词汇表和特殊符号进行训练  
   trainer = trainers.BpeTrainer(  
       vocab_size=8000,  # 新的词汇表大小  
       special_tokens=["[UNK]", "[CLS]", "[SEP]", "[PAD]"]  
   )  
  
   tokenizer.train_from_file("path_to_updated_corpus.txt", trainer)  
  
   # 保存新的 tokenizer 配置文件  
   tokenizer.save("path_to_new_tokenizer.json")  
   ```  
  
#### 5. **验证词汇表和映射是否正确**  
   - 在扩充词汇表后，重要的一步是验证现有词汇的 ID 映射是否保持不变。可以通过以下代码检查：  
     
   ```python  
   # 检查某个原词汇是否映射正确  
   original_word = "hello"  # 这是原始 vocab.json 中的词  
   original_id = vocab.get(original_word)  
   print(f"Original ID for '{original_word}':", original_id)  
     
   # 确保新词汇没有影响原有词汇的 ID  
   new_word = "new_word_1"  
   new_id = vocab.get(new_word)  
   print(f"New ID for '{new_word}':", new_id)  
   ```  
  
#### 6. **加载并使用新的分词器**  
   - 最后，在微调时使用新的词汇表和分词器配置。你可以通过 Hugging Face Transformers 的 `AutoTokenizer` 来加载新创建的 `vocab.json` 和 `tokenizer.json` 文件：  
     
   ```python  
   from transformers import AutoTokenizer  
  
   tokenizer = AutoTokenizer.from_pretrained('path_to_new_tokenizer')  
   model = AutoModelForSequenceClassification.from_pretrained('path_to_your_model')  
  
   # 使用新的分词器进行文本处理  
   inputs = tokenizer("你好，世界！", return_tensors="pt")  
   ```  
  
### 总结：  
1. **扩充时保持原ID不变**：扩充时，将新词汇添加到现有词汇表的末尾，确保原始词汇的 ID 映射不会发生变化。  
2. **避免词汇冲突**：在添加新词汇时，检查是否已有该词汇，避免重复ID。  
3. **更新分词器配置**：根据扩充后的词汇表更新 `tokenizer.json` 配置文件，并确保新的分词器仍然可以正确地处理现有和新加入的词汇。  
4. **验证正确性**：在扩充后，检查原始词汇的 ID 映射是否保持不变，并确保新词汇的 ID 正确分配。  
  
通过这种方法，你可以在扩充词汇表时，避免影响已有词汇的 ID 映射，从而确保模型可以平稳地处理新的中文语料并进行微调。  
    
---  

<b>如何将两个模型的vocab.json及tokenizer.json合并(例如 一个仅支持英文的大模型 和 一个同时支持英文和中文的大模型.), 并且保留其中一个模型的映射ID, 合并时需要避免重复词汇.</b>  
  
为了将两个模型的 `vocab.json` 和 `tokenizer.json` 合并，并且保留其中一个模型的映射 ID，同时避免重复词汇，我们可以按照以下步骤进行操作：

### 步骤概述：
1. **加载两个模型的 `vocab.json` 文件**，并检查词汇映射。
2. **合并词汇表**：将两个模型的词汇表合并，并避免重复的词汇。
3. **保留其中一个模型的 ID 映射**：确保保留第一个模型的词汇 ID 映射。
4. **更新 `tokenizer.json` 配置**：根据新的词汇表更新分词器的配置。
5. **保存新的 `vocab.json` 和 `tokenizer.json` 文件**。

### 具体步骤：

#### 1. **加载原始的 `vocab.json` 文件**
首先，加载两个模型的 `vocab.json` 文件：

```python
import json

# 加载第一个模型的 vocab.json
with open('path_to_model1_vocab.json', 'r', encoding='utf-8') as f:
    vocab1 = json.load(f)

# 加载第二个模型的 vocab.json
with open('path_to_model2_vocab.json', 'r', encoding='utf-8') as f:
    vocab2 = json.load(f)

# 查看各自的词汇映射（可选）
print(list(vocab1.items())[:5])
print(list(vocab2.items())[:5])
```

#### 2. **合并词汇表并避免重复**
合并两个词汇表时，我们需要：
- 保证第一个模型的 ID 映射保持不变。
- 将第二个模型中没有出现在第一个模型中的词汇添加到词汇表中，并为其分配新的 ID。

首先，定义一个合并逻辑：遍历第二个词汇表，只有在该词汇不在第一个词汇表中时，才将其添加。

```python
# 合并词汇表
merged_vocab = vocab1.copy()  # 复制第一个模型的词汇表

# 给第二个模型的词汇分配新的ID，从第一个模型的词汇表ID数开始
start_id = len(vocab1)  # 新词汇的起始ID

# 遍历第二个模型的词汇
for word, word_id in vocab2.items():
    if word not in merged_vocab:  # 如果该词汇没有出现在第一个模型的词汇表中
        merged_vocab[word] = start_id  # 给它分配新的 ID
        start_id += 1

# 查看合并后的词汇表（可选）
print(list(merged_vocab.items())[:5])
```

#### 3. **更新 `tokenizer.json` 配置文件**
合并完词汇表后，我们还需要更新 `tokenizer.json` 配置。`tokenizer.json` 文件包含分词器的类型、特殊符号（如 `[UNK]`, `[CLS]` 等）、以及其他信息。我们将保留第一个模型的分词器配置，同时更新词汇表。

我们可以基于第一个模型的 `tokenizer.json` 文件进行修改，并确保它包含合并后的词汇表。

```python
from tokenizers import Tokenizer, models, pre_tokenizers, decoders, trainers

# 加载第一个模型的 tokenizer.json 配置文件
with open('path_to_model1_tokenizer.json', 'r', encoding='utf-8') as f:
    tokenizer_config1 = json.load(f)

# 创建新的 tokenizer 配置（以第一个模型的配置为基础）
tokenizer = Tokenizer(models.BPE())
tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()
tokenizer.decoder = decoders.BPE()

# 获取第一个模型的特殊符号
special_tokens = tokenizer_config1.get('special_tokens', ["[UNK]", "[CLS]", "[SEP]", "[PAD]"])

# 更新新的分词器配置：词汇表大小，词汇表和特殊符号
new_tokenizer_config = tokenizer_config1.copy()
new_tokenizer_config['vocab_size'] = len(merged_vocab)
new_tokenizer_config['vocab'] = merged_vocab  # 使用新的合并词汇表
new_tokenizer_config['special_tokens'] = special_tokens  # 保持特殊符号

# 保存新的 tokenizer.json 配置文件
with open('new_tokenizer.json', 'w', encoding='utf-8') as f:
    json.dump(new_tokenizer_config, f, ensure_ascii=False, indent=4)
```

#### 4. **保存新的 `vocab.json` 和 `tokenizer.json` 文件**
在合并词汇表并更新分词器配置后，我们需要保存新的 `vocab.json` 和 `tokenizer.json` 文件：

```python
# 保存合并后的 vocab.json 文件
with open('new_vocab.json', 'w', encoding='utf-8') as f:
    json.dump(merged_vocab, f, ensure_ascii=False, indent=4)

# 保存新的 tokenizer.json 文件
with open('new_tokenizer.json', 'w', encoding='utf-8') as f:
    json.dump(new_tokenizer_config, f, ensure_ascii=False, indent=4)
```

#### 5. **验证合并后的词汇表**
在合并和保存文件后，可以验证合并后的词汇表是否没有丢失任何词汇，并确保没有重复的词汇。

```python
# 检查合并后的词汇表是否包含所有词
print(f"合并后词汇表大小: {len(merged_vocab)}")
```

### 注意事项：
1. **词汇冲突处理**：我们通过在合并时检查每个词汇是否已存在，来避免重复词汇。如果有冲突，可以选择跳过或重新映射 ID。
2. **特殊符号**：`tokenizer.json` 中的特殊符号（如 `[UNK]`, `[CLS]`, `[SEP]` 等）需要确保合并后仍然保持一致。
3. **分词器兼容性**：更新后的 `tokenizer.json` 配置文件需要与新的词汇表兼容，确保在微调时不会导致问题。
4. **ID顺序**：保留第一个模型的词汇 ID 映射时，第二个模型的词汇将从第一个模型的 ID 数量开始继续分配，确保不会打乱原有的 ID 映射。

### 总结：
- **合并词汇表时**，首先保留第一个模型的词汇和 ID 映射，并将第二个模型的非重复词汇添加到词汇表中，分配新的 ID。
- **更新 `tokenizer.json` 配置**，以适应新的合并词汇表，同时保留特殊符号和分词器的其他配置。
- **验证合并后的词汇表**，确保没有重复的词汇，并且新的配置不会影响分词器的工作。

这种方法能确保你成功地将两个模型的词汇表合并，并且避免打乱已有的词汇 ID 映射。
  
  
  
#### [期望 PostgreSQL|开源PolarDB 增加什么功能?](https://github.com/digoal/blog/issues/76 "269ac3d1c492e938c0191101c7238216")
  
  
#### [PolarDB 开源数据库](https://openpolardb.com/home "57258f76c37864c6e6d23383d05714ea")
  
  
#### [PolarDB 学习图谱](https://www.aliyun.com/database/openpolardb/activity "8642f60e04ed0c814bf9cb9677976bd4")
  
  
#### [PostgreSQL 解决方案集合](../201706/20170601_02.md "40cff096e9ed7122c512b35d8561d9c8")
  
  
#### [德哥 / digoal's Github - 公益是一辈子的事.](https://github.com/digoal/blog/blob/master/README.md "22709685feb7cab07d30f30387f0a9ae")
  
  
#### [About 德哥](https://github.com/digoal/blog/blob/master/me/readme.md "a37735981e7704886ffd590565582dd0")
  
  
![digoal's wechat](../pic/digoal_weixin.jpg "f7ad92eeba24523fd47a6e1a0e691b59")
  
